{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "675324d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2022-09-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant GRB_ENV. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gurobi.Env(Ptr{Nothing} @0x00007f8eefb97200, false, 0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV, DataFrames, Statistics, StatsBase, Distributions, JuMP, Gurobi, LinearAlgebra\n",
    "\n",
    "const GRB_ENV = Gurobi.Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4fd85013",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SAFI #####\n",
    "\n",
    "X_test_adaptive = CSV.read(\"data/X_test_adaptive.csv\", DataFrame)\n",
    "y_test = CSV.read(\"data/y_test_speed.csv\", DataFrame)\n",
    "# select!(X_test_adaptive, Not([:RANSACRegressor, :GaussianProcessRegressor, :KernelRidge, :Lars, :AdaBoostRegressor,\n",
    "#                      :DummyRegressor, :ExtraTreeRegressor, :Lasso, :LassoLars, :PassiveAggressiveRegressor]))\n",
    "\n",
    "X = X_test_adaptive#[!,1]\n",
    "#X = X[!,[2, 3,5,8,9,11,12,13,25]]\n",
    "y = y_test[!, \"speed\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2f819715",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = size(y)[1]\n",
    "y_safi = (y .- mean(y[1:floor(Int, n/2),:], dims =1))./ std(y[1:floor(Int, n/2),:], dims=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76b3f7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Int64</th><th>DataType</th></tr></thead><tbody><p>6 rows × 7 columns</p><tr><th>1</th><td>NUMTECH_speed</td><td>0.0329076</td><td>-4.63357</td><td>0.0733019</td><td>5.13105</td><td>0</td><td>Float64</td></tr><tr><th>2</th><td>speed</td><td>0.0566938</td><td>-5.07005</td><td>0.104789</td><td>3.51963</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>DT_speed</td><td>0.0672517</td><td>-5.52488</td><td>0.106152</td><td>5.08333</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>TABNET_speed</td><td>0.0081165</td><td>-4.80957</td><td>0.0602244</td><td>3.33884</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>RIDGE_speed</td><td>0.0248569</td><td>-5.41145</td><td>0.114219</td><td>3.64899</td><td>0</td><td>Float64</td></tr><tr><th>6</th><td>LASSO_speed</td><td>0.0176461</td><td>-4.4157</td><td>0.117052</td><td>3.32267</td><td>0</td><td>Float64</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64 & Float64 & Float64 & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & NUMTECH\\_speed & 0.0329076 & -4.63357 & 0.0733019 & 5.13105 & 0 & Float64 \\\\\n",
       "\t2 & speed & 0.0566938 & -5.07005 & 0.104789 & 3.51963 & 0 & Float64 \\\\\n",
       "\t3 & DT\\_speed & 0.0672517 & -5.52488 & 0.106152 & 5.08333 & 0 & Float64 \\\\\n",
       "\t4 & TABNET\\_speed & 0.0081165 & -4.80957 & 0.0602244 & 3.33884 & 0 & Float64 \\\\\n",
       "\t5 & RIDGE\\_speed & 0.0248569 & -5.41145 & 0.114219 & 3.64899 & 0 & Float64 \\\\\n",
       "\t6 & LASSO\\_speed & 0.0176461 & -4.4157 & 0.117052 & 3.32267 & 0 & Float64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m6×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable      \u001b[0m\u001b[1m mean      \u001b[0m\u001b[1m min      \u001b[0m\u001b[1m median    \u001b[0m\u001b[1m max     \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltyp\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol        \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataT\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ NUMTECH_speed  0.0329076  -4.63357  0.0733019  5.13105         0  Float ⋯\n",
       "   2 │ speed          0.0566938  -5.07005  0.104789   3.51963         0  Float\n",
       "   3 │ DT_speed       0.0672517  -5.52488  0.106152   5.08333         0  Float\n",
       "   4 │ TABNET_speed   0.0081165  -4.80957  0.0602244  3.33884         0  Float\n",
       "   5 │ RIDGE_speed    0.0248569  -5.41145  0.114219   3.64899         0  Float ⋯\n",
       "   6 │ LASSO_speed    0.0176461  -4.4157   0.117052   3.32267         0  Float\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(X_test_adaptive[:,2:7] .- y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c87c03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>std</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>6 rows × 3 columns</p><tr><th>1</th><td>NUMTECH_speed</td><td>0.0329076</td><td>1.26841</td></tr><tr><th>2</th><td>speed</td><td>0.0566938</td><td>0.877551</td></tr><tr><th>3</th><td>DT_speed</td><td>0.0672517</td><td>1.02984</td></tr><tr><th>4</th><td>TABNET_speed</td><td>0.0081165</td><td>0.905541</td></tr><tr><th>5</th><td>RIDGE_speed</td><td>0.0248569</td><td>0.999924</td></tr><tr><th>6</th><td>LASSO_speed</td><td>0.0176461</td><td>0.978456</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& variable & mean & std\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & NUMTECH\\_speed & 0.0329076 & 1.26841 \\\\\n",
       "\t2 & speed & 0.0566938 & 0.877551 \\\\\n",
       "\t3 & DT\\_speed & 0.0672517 & 1.02984 \\\\\n",
       "\t4 & TABNET\\_speed & 0.0081165 & 0.905541 \\\\\n",
       "\t5 & RIDGE\\_speed & 0.0248569 & 0.999924 \\\\\n",
       "\t6 & LASSO\\_speed & 0.0176461 & 0.978456 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m6×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable      \u001b[0m\u001b[1m mean      \u001b[0m\u001b[1m std      \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol        \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64  \u001b[0m\n",
       "─────┼────────────────────────────────────\n",
       "   1 │ NUMTECH_speed  0.0329076  1.26841\n",
       "   2 │ speed          0.0566938  0.877551\n",
       "   3 │ DT_speed       0.0672517  1.02984\n",
       "   4 │ TABNET_speed   0.0081165  0.905541\n",
       "   5 │ RIDGE_speed    0.0248569  0.999924\n",
       "   6 │ LASSO_speed    0.0176461  0.978456"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(X_test_adaptive[:,2:7] .- y, :mean, :std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42619d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>std</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>24 rows × 3 columns</p><tr><th>1</th><td>BayesianRidge</td><td>3.01734</td><td>59.6378</td></tr><tr><th>2</th><td>ElasticNet</td><td>1.44953</td><td>64.0703</td></tr><tr><th>3</th><td>ElasticNetCV</td><td>0.931037</td><td>60.2554</td></tr><tr><th>4</th><td>GammaRegressor</td><td>0.057711</td><td>68.6374</td></tr><tr><th>5</th><td>GeneralizedLinearRegressor</td><td>0.582401</td><td>67.0447</td></tr><tr><th>6</th><td>HistGradientBoostingRegressor</td><td>20.8497</td><td>65.9726</td></tr><tr><th>7</th><td>HuberRegressor</td><td>-5.2734</td><td>60.1917</td></tr><tr><th>8</th><td>LarsCV</td><td>1.13659</td><td>59.9771</td></tr><tr><th>9</th><td>Lasso</td><td>1.4169</td><td>59.9211</td></tr><tr><th>10</th><td>LassoCV</td><td>-0.0565766</td><td>59.6822</td></tr><tr><th>11</th><td>LassoLarsCV</td><td>0.0127972</td><td>59.6895</td></tr><tr><th>12</th><td>LassoLarsIC</td><td>-0.0599784</td><td>59.5546</td></tr><tr><th>13</th><td>LinearRegression</td><td>4.21631</td><td>60.2873</td></tr><tr><th>14</th><td>LinearSVR</td><td>-6.30994</td><td>60.2945</td></tr><tr><th>15</th><td>MLPRegressor</td><td>2.65384</td><td>69.6359</td></tr><tr><th>16</th><td>OrthogonalMatchingPursuit</td><td>-4.25956</td><td>59.4915</td></tr><tr><th>17</th><td>OrthogonalMatchingPursuitCV</td><td>-0.633744</td><td>60.0039</td></tr><tr><th>18</th><td>PoissonRegressor</td><td>4.08991</td><td>69.0245</td></tr><tr><th>19</th><td>Ridge</td><td>3.84331</td><td>59.9261</td></tr><tr><th>20</th><td>RidgeCV</td><td>3.21162</td><td>59.7046</td></tr><tr><th>21</th><td>SGDRegressor</td><td>14.5833</td><td>60.9567</td></tr><tr><th>22</th><td>TransformedTargetRegressor</td><td>4.21631</td><td>60.2873</td></tr><tr><th>23</th><td>TweedieRegressor</td><td>0.582401</td><td>67.0447</td></tr><tr><th>24</th><td>LGBMRegressor</td><td>14.6963</td><td>64.1774</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& variable & mean & std\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & BayesianRidge & 3.01734 & 59.6378 \\\\\n",
       "\t2 & ElasticNet & 1.44953 & 64.0703 \\\\\n",
       "\t3 & ElasticNetCV & 0.931037 & 60.2554 \\\\\n",
       "\t4 & GammaRegressor & 0.057711 & 68.6374 \\\\\n",
       "\t5 & GeneralizedLinearRegressor & 0.582401 & 67.0447 \\\\\n",
       "\t6 & HistGradientBoostingRegressor & 20.8497 & 65.9726 \\\\\n",
       "\t7 & HuberRegressor & -5.2734 & 60.1917 \\\\\n",
       "\t8 & LarsCV & 1.13659 & 59.9771 \\\\\n",
       "\t9 & Lasso & 1.4169 & 59.9211 \\\\\n",
       "\t10 & LassoCV & -0.0565766 & 59.6822 \\\\\n",
       "\t11 & LassoLarsCV & 0.0127972 & 59.6895 \\\\\n",
       "\t12 & LassoLarsIC & -0.0599784 & 59.5546 \\\\\n",
       "\t13 & LinearRegression & 4.21631 & 60.2873 \\\\\n",
       "\t14 & LinearSVR & -6.30994 & 60.2945 \\\\\n",
       "\t15 & MLPRegressor & 2.65384 & 69.6359 \\\\\n",
       "\t16 & OrthogonalMatchingPursuit & -4.25956 & 59.4915 \\\\\n",
       "\t17 & OrthogonalMatchingPursuitCV & -0.633744 & 60.0039 \\\\\n",
       "\t18 & PoissonRegressor & 4.08991 & 69.0245 \\\\\n",
       "\t19 & Ridge & 3.84331 & 59.9261 \\\\\n",
       "\t20 & RidgeCV & 3.21162 & 59.7046 \\\\\n",
       "\t21 & SGDRegressor & 14.5833 & 60.9567 \\\\\n",
       "\t22 & TransformedTargetRegressor & 4.21631 & 60.2873 \\\\\n",
       "\t23 & TweedieRegressor & 0.582401 & 67.0447 \\\\\n",
       "\t24 & LGBMRegressor & 14.6963 & 64.1774 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m24×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable                      \u001b[0m\u001b[1m mean       \u001b[0m\u001b[1m std     \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol                        \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────\n",
       "   1 │ BayesianRidge                   3.01734    59.6378\n",
       "   2 │ ElasticNet                      1.44953    64.0703\n",
       "   3 │ ElasticNetCV                    0.931037   60.2554\n",
       "   4 │ GammaRegressor                  0.057711   68.6374\n",
       "   5 │ GeneralizedLinearRegressor      0.582401   67.0447\n",
       "   6 │ HistGradientBoostingRegressor  20.8497     65.9726\n",
       "   7 │ HuberRegressor                 -5.2734     60.1917\n",
       "   8 │ LarsCV                          1.13659    59.9771\n",
       "   9 │ Lasso                           1.4169     59.9211\n",
       "  10 │ LassoCV                        -0.0565766  59.6822\n",
       "  11 │ LassoLarsCV                     0.0127972  59.6895\n",
       "  ⋮  │               ⋮                    ⋮          ⋮\n",
       "  15 │ MLPRegressor                    2.65384    69.6359\n",
       "  16 │ OrthogonalMatchingPursuit      -4.25956    59.4915\n",
       "  17 │ OrthogonalMatchingPursuitCV    -0.633744   60.0039\n",
       "  18 │ PoissonRegressor                4.08991    69.0245\n",
       "  19 │ Ridge                           3.84331    59.9261\n",
       "  20 │ RidgeCV                         3.21162    59.7046\n",
       "  21 │ SGDRegressor                   14.5833     60.9567\n",
       "  22 │ TransformedTargetRegressor      4.21631    60.2873\n",
       "  23 │ TweedieRegressor                0.582401   67.0447\n",
       "  24 │ LGBMRegressor                  14.6963     64.1774\n",
       "\u001b[36m                                            3 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### ENERGY #####\n",
    "X_energy = CSV.read(\"data/energy_predictions_test_val.csv\", DataFrame)\n",
    "y_energy = CSV.read(\"data/energy_y_test_val.csv\", DataFrame, header = 0);\n",
    "describe(X_energy[4000:8000,2:25] .- y_energy[4000:8000,1], :mean, :std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1209af45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>std</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>24 rows × 3 columns</p><tr><th>1</th><td>BayesianRidge</td><td>28.553</td><td>52.4433</td></tr><tr><th>2</th><td>ElasticNet</td><td>30.3495</td><td>56.4427</td></tr><tr><th>3</th><td>ElasticNetCV</td><td>27.5846</td><td>53.5768</td></tr><tr><th>4</th><td>GammaRegressor</td><td>32.5871</td><td>60.4062</td></tr><tr><th>5</th><td>GeneralizedLinearRegressor</td><td>32.3298</td><td>58.7354</td></tr><tr><th>6</th><td>HistGradientBoostingRegressor</td><td>41.7822</td><td>55.1454</td></tr><tr><th>7</th><td>HuberRegressor</td><td>24.0737</td><td>55.4182</td></tr><tr><th>8</th><td>LarsCV</td><td>26.6521</td><td>53.7404</td></tr><tr><th>9</th><td>Lasso</td><td>26.787</td><td>53.6174</td></tr><tr><th>10</th><td>LassoCV</td><td>26.7808</td><td>53.3346</td></tr><tr><th>11</th><td>LassoLarsCV</td><td>26.7963</td><td>53.335</td></tr><tr><th>12</th><td>LassoLarsIC</td><td>26.8104</td><td>53.1768</td></tr><tr><th>13</th><td>LinearRegression</td><td>30.1099</td><td>52.3975</td></tr><tr><th>14</th><td>LinearSVR</td><td>23.871</td><td>55.7251</td></tr><tr><th>15</th><td>MLPRegressor</td><td>43.1145</td><td>54.7437</td></tr><tr><th>16</th><td>OrthogonalMatchingPursuit</td><td>26.2301</td><td>53.5649</td></tr><tr><th>17</th><td>OrthogonalMatchingPursuitCV</td><td>26.6696</td><td>53.7534</td></tr><tr><th>18</th><td>PoissonRegressor</td><td>32.591</td><td>60.9809</td></tr><tr><th>19</th><td>Ridge</td><td>29.5544</td><td>52.2707</td></tr><tr><th>20</th><td>RidgeCV</td><td>28.8434</td><td>52.3718</td></tr><tr><th>21</th><td>SGDRegressor</td><td>35.7161</td><td>51.5023</td></tr><tr><th>22</th><td>TransformedTargetRegressor</td><td>30.1099</td><td>52.3975</td></tr><tr><th>23</th><td>TweedieRegressor</td><td>32.3298</td><td>58.7354</td></tr><tr><th>24</th><td>LGBMRegressor</td><td>36.5706</td><td>54.7452</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& variable & mean & std\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & BayesianRidge & 28.553 & 52.4433 \\\\\n",
       "\t2 & ElasticNet & 30.3495 & 56.4427 \\\\\n",
       "\t3 & ElasticNetCV & 27.5846 & 53.5768 \\\\\n",
       "\t4 & GammaRegressor & 32.5871 & 60.4062 \\\\\n",
       "\t5 & GeneralizedLinearRegressor & 32.3298 & 58.7354 \\\\\n",
       "\t6 & HistGradientBoostingRegressor & 41.7822 & 55.1454 \\\\\n",
       "\t7 & HuberRegressor & 24.0737 & 55.4182 \\\\\n",
       "\t8 & LarsCV & 26.6521 & 53.7404 \\\\\n",
       "\t9 & Lasso & 26.787 & 53.6174 \\\\\n",
       "\t10 & LassoCV & 26.7808 & 53.3346 \\\\\n",
       "\t11 & LassoLarsCV & 26.7963 & 53.335 \\\\\n",
       "\t12 & LassoLarsIC & 26.8104 & 53.1768 \\\\\n",
       "\t13 & LinearRegression & 30.1099 & 52.3975 \\\\\n",
       "\t14 & LinearSVR & 23.871 & 55.7251 \\\\\n",
       "\t15 & MLPRegressor & 43.1145 & 54.7437 \\\\\n",
       "\t16 & OrthogonalMatchingPursuit & 26.2301 & 53.5649 \\\\\n",
       "\t17 & OrthogonalMatchingPursuitCV & 26.6696 & 53.7534 \\\\\n",
       "\t18 & PoissonRegressor & 32.591 & 60.9809 \\\\\n",
       "\t19 & Ridge & 29.5544 & 52.2707 \\\\\n",
       "\t20 & RidgeCV & 28.8434 & 52.3718 \\\\\n",
       "\t21 & SGDRegressor & 35.7161 & 51.5023 \\\\\n",
       "\t22 & TransformedTargetRegressor & 30.1099 & 52.3975 \\\\\n",
       "\t23 & TweedieRegressor & 32.3298 & 58.7354 \\\\\n",
       "\t24 & LGBMRegressor & 36.5706 & 54.7452 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m24×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable                      \u001b[0m\u001b[1m mean    \u001b[0m\u001b[1m std     \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol                        \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼─────────────────────────────────────────────────\n",
       "   1 │ BayesianRidge                  28.553   52.4433\n",
       "   2 │ ElasticNet                     30.3495  56.4427\n",
       "   3 │ ElasticNetCV                   27.5846  53.5768\n",
       "   4 │ GammaRegressor                 32.5871  60.4062\n",
       "   5 │ GeneralizedLinearRegressor     32.3298  58.7354\n",
       "   6 │ HistGradientBoostingRegressor  41.7822  55.1454\n",
       "   7 │ HuberRegressor                 24.0737  55.4182\n",
       "   8 │ LarsCV                         26.6521  53.7404\n",
       "   9 │ Lasso                          26.787   53.6174\n",
       "  10 │ LassoCV                        26.7808  53.3346\n",
       "  11 │ LassoLarsCV                    26.7963  53.335\n",
       "  ⋮  │               ⋮                   ⋮        ⋮\n",
       "  15 │ MLPRegressor                   43.1145  54.7437\n",
       "  16 │ OrthogonalMatchingPursuit      26.2301  53.5649\n",
       "  17 │ OrthogonalMatchingPursuitCV    26.6696  53.7534\n",
       "  18 │ PoissonRegressor               32.591   60.9809\n",
       "  19 │ Ridge                          29.5544  52.2707\n",
       "  20 │ RidgeCV                        28.8434  52.3718\n",
       "  21 │ SGDRegressor                   35.7161  51.5023\n",
       "  22 │ TransformedTargetRegressor     30.1099  52.3975\n",
       "  23 │ TweedieRegressor               32.3298  58.7354\n",
       "  24 │ LGBMRegressor                  36.5706  54.7452\n",
       "\u001b[36m                                         3 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(abs.(X_energy[4000:8000,2:25] .- y_energy[4000:8000,1]), :mean, :std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c7b9dfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8494, 1)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(y_safi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bb4759a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3347"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4247-900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "bc758d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 900\n",
    "Nt = 100\n",
    "p = 10\n",
    "\n",
    "δ = 0.5\n",
    "δ_pert = 0.5\n",
    "σ_pert = 1\n",
    "#y = randn(N0+Nt)\n",
    "n0 = floor(Int, size(y_safi)[1]/2)\n",
    "y_true = y_safi[n0-N0:n0+Nt-1]\n",
    "y = copy(y_true)\n",
    "d = Uniform(-δ, δ)\n",
    "biases = rand(d, p)\n",
    "variances = rand(p)\n",
    "gradual_after = false\n",
    "gradual_before = true\n",
    "\n",
    "#Create the features X by taking y and adding noise.\n",
    "X = zeros(N0+Nt, p)\n",
    "\n",
    "for i=1:p\n",
    "    d = Normal(biases[i], variances[i])\n",
    "    X[:,i] = y.+ rand(d, N0+Nt)\n",
    "end\n",
    "\n",
    "#### Perturbations on base learners because of data drift\n",
    "d = Normal(0, δ_pert)\n",
    "biases_perturb = rand(d, p)\n",
    "d = Uniform(0, σ_pert)\n",
    "variances_perturb = rand(d, p)\n",
    "\n",
    "if gradual_after\n",
    "    for i=1:p\n",
    "        d = Normal(biases_perturb[i], variances_perturb[i])\n",
    "        #We add the perturbation for each model \n",
    "        #We make the perturbations more and more intense across time\n",
    "        X[N0+1:N0+Nt,i] = X[N0+1:N0+Nt,i].+rand(d, Nt).*[t/Nt for t=1:Nt]\n",
    "    end\n",
    "end\n",
    "\n",
    "if gradual_before\n",
    "    for i=1:p\n",
    "        for t=1:Nt\n",
    "            #We compute the perturbation for each model \n",
    "            #We make the perturbations more and more intense across time\n",
    "            d = Normal(t/Nt*δ_pert, t/Nt*σ_pert)\n",
    "            X[N0+t,i] = X[N0+t,i]+rand(d)\n",
    "        end\n",
    "    end\n",
    "else\n",
    "    for i=1:p\n",
    "        d = Normal(biases_perturb[i], variances_perturb[i])\n",
    "        #We add the perturbation for each model \n",
    "        #We make the perturbations more and more intense across time\n",
    "        X[N0+1:N0+Nt,i] = X[N0+1:N0+Nt,i].+rand(d, Nt)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "2f90335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Perturbations on y\n",
    "perturb_y = true\n",
    "perturb_y_norm = true\n",
    "y_bias = -0.1\n",
    "y_var = 0.5\n",
    "\n",
    "if perturb_y\n",
    "    if perturb_y_norm\n",
    "        d = Normal(y_bias, y_var)\n",
    "        y[N0+1:N0+Nt] = y[N0+1:N0+Nt] .+ rand(d, Nt)\n",
    "    else\n",
    "        d = Uniform(-y_bias, y_bias)\n",
    "        y[N0+1:N0+Nt] = y[N0+1:N0+Nt] .+ rand(d, Nt)\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "a685535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12)\n",
      "### β0 Baseline ###\n",
      "MAE Baseline: 0.2926726628350745\n",
      "CVAR 0.05 :0.9516292171887668\n",
      "CVAR 0.15 :0.7810909687051267\n",
      "R2 : 0.5596709463338037\n",
      "\n",
      "### β0 Baseline Retrained ###\n",
      "MAE Baseline: 0.2366397874232855\n",
      "CVAR 0.05 :0.7800146141813811\n",
      "CVAR 0.15 :0.5965179535644068\n",
      "R2 : 0.7228504073292392\n",
      "\n",
      "### β0 V0 Linear rule Adaptive ###\n",
      "MAE Baseline: 0.2906279546385435\n",
      "CVAR 0.05 :0.9926259187816489\n",
      "CVAR 0.15 :0.7719802373504452\n",
      "R2 : 0.5620597548454742\n",
      "\n",
      "### β0 V0 Linear rule Adaptive Retrained ###\n",
      "MAE Baseline: 0.25762201468040113\n",
      "CVAR 0.05 :0.8801040054421982\n",
      "CVAR 0.15 :0.6322623660372929\n",
      "R2 : 0.6717637589787904\n",
      "\n",
      "### β0 Adaptive ###\n",
      "MAE 0: 0.24097862855768679\n",
      "CVAR 0.05 :0.8013637405312338\n",
      "CVAR 0.15 :0.6107513493064963\n",
      "R2 : 0.7087045512821339\n",
      "\n",
      "### βt Adaptive ###\n",
      "MAE t: 0.2068937242192836\n",
      "CVAR 0.05 :0.7503999427946417\n",
      "CVAR 0.15 :0.5723872188663088\n",
      "R2 : 0.7651533718100071\n"
     ]
    }
   ],
   "source": [
    "split_ = N0/(N0+Nt)\n",
    "past = 10\n",
    "num_past = floor(Int, Nt/past)\n",
    "\n",
    "last_yT = false\n",
    "max_cuts = 10\n",
    "verbose = false\n",
    "\n",
    "uncertainty = 0.#1\n",
    "δ_inf = 0.02\n",
    "ϵ_inf = 0.02\n",
    "\n",
    "ϵ_l2 = 0.05\n",
    "δ_l2 = 0.05\n",
    "\n",
    "reg = 1/(past*num_past)\n",
    "ρ = 0.1\n",
    "val = Nt-1; #n-split_index;\n",
    "\n",
    "fix_β0 = false\n",
    "more_data_for_β0 = false\n",
    "benders = false\n",
    "ridge = true\n",
    "\n",
    "eval_method(X, y, y_true, split_, past, num_past, val, uncertainty, ϵ_inf, δ_inf, last_yT, \n",
    "        ϵ_l2, δ_l2, ρ, reg, max_cuts, verbose, \n",
    "        fix_β0, more_data_for_β0, benders, ridge, linear_adapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1d6a6fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### β0 Baseline ###\n",
      "MAE Baseline: 0.23292285353920797\n",
      "CVAR 0.05 :0.8248781648196017\n",
      "CVAR 0.15 :0.6146000206902295\n",
      "R2 : 0.7231483653113947\n",
      "\n",
      "### β0 Baseline Retrained ###\n",
      "MAE Baseline: 0.24235092030157646\n",
      "CVAR 0.05 :0.8089715074444982\n",
      "CVAR 0.15 :0.6809646681941373\n",
      "R2 : 0.686903282141481\n",
      "\n",
      "### β0 Adaptive ###\n",
      "MAE 0: 0.20902448857494324\n",
      "CVAR 0.05 :0.7269394354418665\n",
      "CVAR 0.15 :0.5684294172269506\n",
      "R2 : 0.7696332848022117\n",
      "\n",
      "### βt Adaptive ###\n",
      "MAE t: 0.21357895684413064\n",
      "CVAR 0.05 :0.7495916668128307\n",
      "CVAR 0.15 :0.5651026247906372\n",
      "R2 : 0.76674460974741\n"
     ]
    }
   ],
   "source": [
    "split_ = N0/(N0+Nt)\n",
    "past = 20\n",
    "num_past = floor(Int, Nt/past)\n",
    "\n",
    "last_yT = false\n",
    "max_cuts = 10\n",
    "verbose = false\n",
    "\n",
    "uncertainty = 0.#1\n",
    "δ_inf = 0.02\n",
    "ϵ_inf = 0.02\n",
    "\n",
    "ϵ_l2 = 0.05\n",
    "δ_l2 = 0.05\n",
    "\n",
    "reg = 1/(past*num_past)\n",
    "ρ = 0.1\n",
    "val = Nt-1; #n-split_index;\n",
    "\n",
    "fix_β0 = false\n",
    "more_data_for_β0 = false\n",
    "benders = false\n",
    "ridge = true\n",
    "\n",
    "eval_method(X, y, y_true, split_, past, num_past, val, uncertainty, ϵ_inf, δ_inf, last_yT, \n",
    "        ϵ_l2, δ_l2, ρ, reg, max_cuts, verbose, \n",
    "        fix_β0, more_data_for_β0, benders, ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de22ec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "master_problem (generic function with 8 methods)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_Y(X, t)\n",
    "    \"\n",
    "    Create the vector of data for the dual problem\n",
    "    \"\n",
    "    T, p = size(X)\n",
    "    Y = zeros(1,T*p)\n",
    "    Y[(t-1)*p+1:t*p] = X[t,:]\n",
    "    return Y\n",
    "end\n",
    "\n",
    "function get_Z(X)\n",
    "    T, p = size(X)\n",
    "    Z = zeros(T, T*p)\n",
    "    for t=1:T\n",
    "        Z[t,:] = get_Y(X,t)\n",
    "    end\n",
    "    return Z\n",
    "end\n",
    "\n",
    "function get_A(X, t)\n",
    "    T, p = size(X)\n",
    "    A = zeros(p,T*p)\n",
    "    A[:,(t-1)*p+1:t*p] = 1 * Matrix(I, p, p)\n",
    "    return A\n",
    "end\n",
    "\n",
    "\n",
    "function solve_model_benders(m)\n",
    "    \"Solve the Benders master problem\n",
    "    \"\n",
    "    optimize!(m)\n",
    "    U_OA = objective_value(m)\n",
    "    #println(\"U_OA from solve \", U_OA)\n",
    "    return value.(m[:β]), value.(m[:α]), U_OA\n",
    "end\n",
    "\n",
    "function S_primal(X, y, β0, epsilon, delta)\n",
    "\n",
    "    n, p = size(X)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, β[i=1:n,j=1:p])\n",
    "    @variable(model, b[i=1:n]>=0)\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Min, sum(b[i] for i=1:n))\n",
    "\n",
    "    #@constraint(model,[i=1:n], y .- dot(X,β) .<= b)\n",
    "    #@constraint(model,[i=1:n],-y .+ dot(X,β) .<= b)\n",
    "\n",
    "\n",
    "    @constraint(model, res_plus[i=1:n],  - y[i] + dot(X[i,:],β[i,:]) <= b[i])\n",
    "    @constraint(model, res_minus[i=1:n],  y[i] - dot(X[i,:],β[i,:]) <= b[i])\n",
    "\n",
    "    @constraint(model, diff_plus[i=2:n],   β[i,:] .- β[i-1,:] .<= delta)\n",
    "    @constraint(model, diff_minus[i=2:n], - β[i,:] .+ β[i-1,:] .<= delta)\n",
    "\n",
    "    @constraint(model, diff_0_plus[i=1:n],   β[i,:] .- β0 .<= epsilon)\n",
    "    @constraint(model, diff_0_minus[i=1:n], - β[i,:] .+ β0 .<= epsilon)\n",
    "\n",
    "    optimize!(model);\n",
    "\n",
    "    return objective_value(model), getvalue.(β)\n",
    "end\n",
    "\n",
    "\n",
    "function R(X, D_min, D_max, β0, epsilon, delta)\n",
    "    \"\n",
    "    Full dual problem\n",
    "    \"\n",
    "    T, p = size(X)\n",
    "    Z = get_Z(X)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))#Model(with_optimizer(Gurobi.Optimizer))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "    set_optimizer_attribute(model, \"NonConvex\", 2)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, λ[i=1:2, j=1:T] >= 0)\n",
    "    @variable(model, ν[i=1:2, j=1:T-1, k=1:p]>=0)\n",
    "    @variable(model, μ[i=1:2, j=1:T, k=1:p]>=0)\n",
    "\n",
    "    @variable(model, y[j=1:T])\n",
    "\n",
    "\n",
    "    @constraint(model,[t=1:T], λ[1,:] .+ λ[2,:] .== 1)\n",
    "\n",
    "\n",
    "    @constraint(model, transpose(λ[2,:])*Z-transpose(λ[1,:])*Z\n",
    "                        + sum(transpose(ν[1,t,:])*(get_A(X, t+1).-get_A(X, t)) for t=1:T-1)\n",
    "                        + sum(transpose(ν[2,t,:])*(-get_A(X, t+1).+get_A(X, t)) for t=1:T-1)\n",
    "                        + sum(transpose(μ[1,t,:])*get_A(X,t) for t=1:T)\n",
    "                        - sum(transpose(μ[2,t,:])*get_A(X,t) for t=1:T) .== 0)\n",
    "\n",
    "    #y in uncertainty set\n",
    "    @constraint(model, [1:T], D_min .<= y)\n",
    "    @constraint(model, [1:T], y .<= D_max)\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Max, 2*dot(λ[1,:],y) - sum(y)\n",
    "                            - delta * sum(sum(ν[1,t,i]+ν[2,t,i] for i=1:p) for t=1:T-1)\n",
    "                            - sum(dot(epsilon .+ β0, μ[1,t,:]) for t = 1:T)\n",
    "                            - sum(dot(epsilon .- β0, μ[2,t,:]) for t = 1:T)) #\n",
    "    optimize!(model)\n",
    "    return objective_value(model), getvalue.(y), getvalue.(λ), getvalue.(ν), getvalue.(μ)\n",
    "end\n",
    "\n",
    "function master_problem(X0, Xt, y0, D_min, D_max, threshold = 0.1, epsilon = 0.1, delta = 0.1, reg = 1, ρ = 1, max_cuts = 10, verbose=0)\n",
    "    n, p = size(X0)\n",
    "    T, p = size(Xt)\n",
    "    #Z = get_Z(X0)\n",
    "    L_BD = -10000\n",
    "    U_BD = 10000\n",
    "    cuts = 0\n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))#Model(with_optimizer(Gurobi.Optimizer))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, α)\n",
    "    @variable(model, β[j=1:p])\n",
    "\n",
    "    #Warm start for β\n",
    "\n",
    "    β_val0 = l2_regression(X0, y0, ρ)#Random.rand(p)#\n",
    "    #β_val0 = Random.rand(p)\n",
    "\n",
    "\n",
    "    #Initialization\n",
    "    _, y_val0, λ_val0, ν_val0, μ_val0 = R(Xt, D_min, D_max, β_val0, epsilon, delta)\n",
    "\n",
    "    #First constraint\n",
    "    @constraint(model, α >= 2*dot(λ_val0[1,:],y_val0) - sum(y_val0)\n",
    "                            - delta * sum(sum(ν_val0[1,t,i]+ν_val0[2,t,i] for i=1:p) for t=1:T-1)\n",
    "                            - sum(dot(epsilon .+ β, μ_val0[1,t,:]) for t = 1:T)\n",
    "                            - sum(dot(epsilon .- β, μ_val0[2,t,:]) for t = 1:T))\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Min, 1/n*sum((y0[i]-sum(X0[i,j]*β[j] for j=1:p))^2 for i=1:n) + reg*α + ρ*sum(β[j]^2 for j=1:p))\n",
    "\n",
    "    while cuts < max_cuts && U_BD - L_BD > threshold\n",
    "        if verbose\n",
    "            println(\"Lower: \", L_BD, \" Upper: \", U_BD)\n",
    "        end\n",
    "        cuts += 1\n",
    "\n",
    "        #Solve current Master Problem\n",
    "        β_val, α_val, L_BD = solve_model_benders(model)\n",
    "        U_OA, y_val, λ_val, ν_val, μ_val = R(Xt, D_min, D_max, β_val, epsilon, delta)\n",
    "\n",
    "        U_BD = 1/n*sum((y0[i]-sum(X0[i,j]*β_val[j] for j=1:p))^2 for i=1:n) + reg*U_OA + ρ*sum(β_val[j]^2 for j=1:p)\n",
    "\n",
    "        if U_BD - L_BD > threshold\n",
    "            @constraint(model, α >= 2*dot(λ_val[1,:],y_val) - sum(y_val)\n",
    "                            - delta * sum(sum(ν_val[1,t,i]+ν_val[2,t,i] for i=1:p) for t=1:T-1)\n",
    "                            - sum(dot(epsilon .+ β, μ_val[1,t,:]) for t = 1:T)\n",
    "                            - sum(dot(epsilon .- β, μ_val[2,t,:]) for t = 1:T))\n",
    "            if verbose\n",
    "                println(\"Cut added\")\n",
    "                println(\"y_val: \", y_val)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    optimize!(model)\n",
    "    β_val, α_val, L_BD = solve_model_benders(model)\n",
    "    U_OA, y_val, λ_val, ν_val, μ_val = R(Xt, D_min, D_max, β_val, epsilon, delta)\n",
    "    if verbose\n",
    "        println(\"Final model Obj value: \", objective_value(model))\n",
    "        println(\"Lower: \", L_BD, \" Upper: \", U_BD)\n",
    "        println(\"Final y: \", y_val)\n",
    "    end\n",
    "    return objective_value(model), getvalue.(β), getvalue.(α), y_val#, getvalue.(ν), getvalue.(μ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6507fc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l2_regression (generic function with 1 method)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function R2(y_true, y_test)\n",
    "    SSR = sum(abs2.(y_true.-y_test))\n",
    "    SST = sum(abs2.(y_true.-mean(y_true)))\n",
    "    return 1 - SSR/SST\n",
    "end\n",
    "\n",
    "function R2_err(err, y_true)\n",
    "    SSR = sum(abs2.(err))\n",
    "    SST = sum(abs2.(y_true.-mean(y_true)))\n",
    "    return 1 - SSR/SST\n",
    "end\n",
    "\n",
    "\n",
    "function compute_CVaR(errs, α_risk)\n",
    "#     '''\n",
    "#     Compute the Conditional Value at Risk\n",
    "#     :param X: Either the data matrix X or the errors\n",
    "#     :param y: the target values\n",
    "#     :param alpha: the risk value\n",
    "#     :param beta:\n",
    "#     :param b0:\n",
    "#     :param errs: whether you want to input the data matrix X or the errors\n",
    "#     :return: the model and the CVaR\n",
    "#     '''\n",
    "    n = size(errs)[1]\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, τ)\n",
    "    @variable(model, z[1:n] >= 0)\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Min, sum(z)/(α_risk * n) + τ)\n",
    "\n",
    "    @constraint(model, [1:n], errs .- τ .<= z)\n",
    "\n",
    "    optimize!(model)\n",
    "\n",
    "    return objective_value(model)\n",
    "end\n",
    "\n",
    "\n",
    "function eval_method(X, y, y_true, split_, past, num_past, val, uncertainty, ϵ_inf, δ_inf, last_yT,\n",
    "        ϵ_l2, δ_l2, ρ, reg, max_cuts, verbose,\n",
    "        fix_β0, more_data_for_β0, benders, ridge)\n",
    "\n",
    "    threshold_benders = 0.01\n",
    "    n, p = size(X)\n",
    "    split_index = floor(Int,n*split_)\n",
    "    #TODO change spli_index with max(split inex, 1)\n",
    "    X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, split_index-num_past*past+1, num_past*past, val, uncertainty, last_yT)\n",
    "\n",
    "    β_list0 = zeros(val, p)\n",
    "    β_listt = zeros(val, p)\n",
    "    β_listl2 = zeros(val, p)\n",
    "    β_l2_init = l2_regression(X0,y0,ρ);\n",
    "    for s=1:val\n",
    "\n",
    "        if more_data_for_β0\n",
    "            X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, split_index-num_past*past+1, s+(num_past-1)*past, past-1, uncertainty, last_yT)\n",
    "        else\n",
    "            X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, s+split_index-num_past*past+1, (num_past-1)*past, past-1, uncertainty, last_yT)\n",
    "        end\n",
    "\n",
    "\n",
    "        if benders\n",
    "            ##TODO handle fix_beta0\n",
    "            obj, β0_val, α, y_val = master_problem(X0, Xt, y0, D_min, D_max, threshold_benders, ϵ_inf, δ_inf, reg, ρ, max_cuts, verbose)\n",
    "            _, βt_val = S_primal(Xt, y_val, β0_val, ϵ_inf, δ_inf);\n",
    "        else\n",
    "            if ridge\n",
    "                obj, βt_val, β0_val = master_primal_l2_ridge(X0, Xt, y0, D_min, D_max, ϵ_inf, δ_inf, reg, ρ, ϵ_l2, δ_l2, fix_β0, β_l2_init)\n",
    "            else\n",
    "                obj, βt_val, β0_val = master_primal_l2(X0, Xt, y0, D_min, D_max, ϵ_inf, δ_inf, reg, ρ, ϵ_l2, δ_l2, fix_β0, β_l2_init)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        β_listt[s,:] = βt_val[past-1,:]\n",
    "        β_list0[s,:] = β0_val\n",
    "        β_l2 = l2_regression(X0,y0,ρ);\n",
    "        β_listl2[s,:] = β_l2\n",
    "\n",
    "    end\n",
    "\n",
    "    X0, y0, Xt, yt, _, D_min, D_max = prepare_data_from_y(X, y, 1, split_index, val, uncertainty, last_yT)\n",
    "    _, _, _, _, yt_true, _, _ = prepare_data_from_y(X, y_true, 1, split_index, val, uncertainty, last_yT)\n",
    "\n",
    "    err_0 = [abs(yt_true[s]-dot(Xt[s,:],β_list0[s,:])) for s=1:val]\n",
    "    err_t = [abs(yt_true[s]-dot(Xt[s,:],β_listt[s,:])) for s=1:val]\n",
    "    err_baseline = [abs(yt_true[s]-dot(Xt[s,:],β_l2_init)) for s=1:val]\n",
    "    err_l2 = [abs(yt_true[s]-dot(Xt[s,:],β_listl2[s,:])) for s=1:val]\n",
    "\n",
    "    println(\"\\n### β0 Baseline ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_baseline))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_baseline, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_baseline, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_baseline, yt_true))\n",
    "\n",
    "    println(\"\\n### β0 Baseline Retrained ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_l2))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_l2, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_l2, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_l2, yt_true))\n",
    "\n",
    "    println(\"\\n### β0 Adaptive ###\")\n",
    "    println(\"MAE 0: \", mean(err_0))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_0, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_0, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_0, yt_true))\n",
    "\n",
    "    println(\"\\n### βt Adaptive ###\")\n",
    "    println(\"MAE t: \", mean(err_t))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_t, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_t, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_t, yt_true))\n",
    "end\n",
    "\n",
    "\n",
    "function prepare_data_from_y(X, y, n0, n, m, uncertainty, last_yT = false)\n",
    "\n",
    "    X0 = Matrix(X[n0:n0+n,:])\n",
    "    X0[:,1] = ones(n+1)\n",
    "    y0 = y[n0:n0+n,:][:]\n",
    "\n",
    "    yt_true = y[n0+n+1:n0+n+m,:][:]\n",
    "    Xt = Matrix(X[n0+n+1:n0+n+m,:])\n",
    "    Xt[:,1] = ones(m)\n",
    "    yt = yt_true\n",
    "    if last_yT\n",
    "        yt_true[m] = mean(Xt[m])\n",
    "    end\n",
    "\n",
    "    D_min = yt .- uncertainty.*abs.(yt)\n",
    "    D_max = yt .+ uncertainty.*abs.(yt)\n",
    "\n",
    "    return X0, y0, Xt, yt, yt_true, D_min, D_max\n",
    "end\n",
    "\n",
    "\n",
    "function l2_regression(X, y, rho; solver_output=0)\n",
    "    n,p = size(X)\n",
    "\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", solver_output)\n",
    "    set_optimizer_attribute(model, \"NonConvex\", 2)\n",
    "\n",
    "    @variable(model,beta[j=1:p])\n",
    "    @variable(model, sse>=0)\n",
    "    #@variable(model, reg>=0)\n",
    "    @constraint(model, sum((y[i]-sum(X[i,j]*beta[j] for j=1:p))^2 for i=1:n) <= sse)\n",
    "    #@constraint(model, sum(beta[j]^2 for j=1:p)<=reg)\n",
    "    @objective(model,Min, 1/n*sse + rho*sum(beta[j]^2 for j=1:p))\n",
    "\n",
    "    optimize!(model)\n",
    "    #println(\"Obj \", objective_value(model))\n",
    "    return value.(beta)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e0e484ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "master_primal_l2_ridge (generic function with 3 methods)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "function master_primal_l2(X0, Xt, y0, Dmin, Dmax, epsilon, delta, reg, ρ, ϵ_l2, δ_l2, β0_fix = false, β0_val = 0)\n",
    "\n",
    "    M = 1000\n",
    "    n0, p = size(X0)\n",
    "    n, _ = size(Xt)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, β[i=1:n,j=1:p])\n",
    "    @variable(model, b[i=1:n]>=0)\n",
    "    @variable(model, β0[j=1:p])\n",
    "    @variable(model, z[i=1:n], Bin)\n",
    "    @variable(model, y[i=1:n])\n",
    "    # Add objective\n",
    "    @objective(model, Min, 1/n0*sum((y0[i]-sum(X0[i,j]*β0[j] for j=1:p))^2 for i=1:n0)\n",
    "        + reg*sum(b[i] for i=1:n) + ρ*sum(β0[j]^2 for j=1:p))\n",
    "\n",
    "    #@constraint(model,[i=1:n], y .- dot(X,β) .<= b)\n",
    "    #@constraint(model,[i=1:n],-y .+ dot(X,β) .<= b)\n",
    "\n",
    "    if β0_fix\n",
    "        @constraint(model, β0 .== β0_val)\n",
    "    end\n",
    "    @constraint(model, res_plus_min[i=1:n],  - y[i] + dot(Xt[i,:],β[i,:]) <= b[i])\n",
    "    @constraint(model, res_minus_min[i=1:n],  y[i] - dot(Xt[i,:],β[i,:]) <= b[i])\n",
    "\n",
    "    @constraint(model,  y .== Dmax.*z + Dmin.*(1 .-z))\n",
    "    #@constraint(model,  y .== Dmax.*(1 .-z) + Dmin.*z)\n",
    "\n",
    "    #@constraint(model, bigM[i=1:n], (dot(Xt[i,:],β[i,:])-Dmin[i])^2 + M*z[i] >= (dot(Xt[i,:],β[i,:])-Dmax[i])^2)\n",
    "    @constraint(model, bigM[i=1:n], (dot(Xt[i,:],β[i,:]) - Dmin[i])^2 - (dot(Xt[i,:],β[i,:])-Dmax[i])^2 + M*z[i] >= 0)\n",
    "\n",
    "\n",
    "    #@constraint(model, bigM2[i=1:n],  (dot(Xt[i,:],β[i,:])-Dmax[i])^2 + M*(1-z[i]) >= (dot(Xt[i,:],β[i,:])-Dmin[i])^2)\n",
    "\n",
    "    @constraint(model, diff_plus[i=2:n],   β[i,:] .- β[i-1,:] .<= delta)\n",
    "    @constraint(model, diff_minus[i=2:n], - β[i,:] .+ β[i-1,:] .<= delta)\n",
    "\n",
    "    @constraint(model, diff_0_plus[i=1:n],   β[i,:] .- β0 .<= epsilon)\n",
    "    @constraint(model, diff_0_minus[i=1:n], - β[i,:] .+ β0 .<= epsilon)\n",
    "\n",
    "    @constraint(model, sq_0[i=1:n], sum((β[i,:] .- β0).^2) .<= ϵ_l2)\n",
    "    @constraint(model, sq_t[i=2:n], sum((β[i,:] .- β[i-1,:]).^2) .<= δ_l2)\n",
    "\n",
    "    #@constraint(model, abs_0[i=1:n], sum(abs.(β[i,:] .- β0)) .<= ϵ_l1)\n",
    "    #@constraint(model, abs_t[i=2:n], sum(abs.(β[i,:] .- β[i-1,:])) .<= δ_l1)\n",
    "\n",
    "    optimize!(model);\n",
    "    #println(\"SUM \", sum(getvalue.(b)))\n",
    "    #println(\"Y \", getvalue.(y))\n",
    "    return objective_value(model), getvalue.(β), getvalue.(β0)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function master_primal_l2_ridge(X0, Xt, y0, Dmin, Dmax, epsilon, delta, reg, ρ, ϵ_l2, δ_l2, β0_fix = false, β0_val = 0)\n",
    "\n",
    "    M = 1000\n",
    "    n0, p = size(X0)\n",
    "    n, _ = size(Xt)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, β[i=1:n,j=1:p])\n",
    "    @variable(model, β0[j=1:p])\n",
    "    @variable(model, z[i=1:n], Bin)\n",
    "    @variable(model, y[i=1:n])\n",
    "    # Add objective\n",
    "    @objective(model, Min, 1/n0*sum((y0[i]-sum(X0[i,j]*β0[j] for j=1:p))^2 for i=1:n0)\n",
    "        + reg*sum((y[i] - dot(Xt[i,:],β[i,:]))^2 for i=1:n) + ρ*sum(β0[j]^2 for j=1:p))\n",
    "\n",
    "    #@constraint(model,[i=1:n], y .- dot(X,β) .<= b)\n",
    "    #@constraint(model,[i=1:n],-y .+ dot(X,β) .<= b)\n",
    "\n",
    "    if β0_fix\n",
    "        @constraint(model, β0 .== β0_val)\n",
    "    end\n",
    "\n",
    "    @constraint(model,  y .== Dmax.*z + Dmin.*(1 .-z))\n",
    "    #@constraint(model,  y .== Dmax.*(1 .-z) + Dmin.*z)\n",
    "\n",
    "    #@constraint(model, bigM[i=1:n], (dot(Xt[i,:],β[i,:])-Dmin[i])^2 + M*z[i] >= (dot(Xt[i,:],β[i,:])-Dmax[i])^2)\n",
    "    @constraint(model, bigM[i=1:n], (dot(Xt[i,:],β[i,:]) - Dmin[i])^2 - (dot(Xt[i,:],β[i,:])-Dmax[i])^2 + M*z[i] >= 0)\n",
    "\n",
    "\n",
    "    #@constraint(model, bigM2[i=1:n],  (dot(Xt[i,:],β[i,:])-Dmax[i])^2 + M*(1-z[i]) >= (dot(Xt[i,:],β[i,:])-Dmin[i])^2)\n",
    "\n",
    "    @constraint(model, diff_plus[i=2:n],   β[i,:] .- β[i-1,:] .<= delta)\n",
    "    @constraint(model, diff_minus[i=2:n], - β[i,:] .+ β[i-1,:] .<= delta)\n",
    "\n",
    "    @constraint(model, diff_0_plus[i=1:n],   β[i,:] .- β0 .<= epsilon)\n",
    "    @constraint(model, diff_0_minus[i=1:n], - β[i,:] .+ β0 .<= epsilon)\n",
    "\n",
    "    @constraint(model, sq_0[i=1:n], sum((β[i,:] .- β0).^2) .<= ϵ_l2)\n",
    "    @constraint(model, sq_t[i=2:n], sum((β[i,:] .- β[i-1,:]).^2) .<= δ_l2)\n",
    "\n",
    "    #@constraint(model, abs_0[i=1:n], sum(abs.(β[i,:] .- β0)) .<= ϵ_l1)\n",
    "    #@constraint(model, abs_t[i=2:n], sum(abs.(β[i,:] .- β[i-1,:])) .<= δ_l1)\n",
    "\n",
    "    optimize!(model);\n",
    "    #println(\"SUM \", sum(getvalue.(b)))\n",
    "    #println(\"Y \", getvalue.(y))\n",
    "    return objective_value(model), getvalue.(β), getvalue.(β0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ecae3",
   "metadata": {},
   "source": [
    "# Linear rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "69b65ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adaptive_ridge_regression_standard (generic function with 1 method)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function adaptive_ridge_regression_standard(X, y, ρ_β0, ρ_V0, T)\n",
    "    \n",
    "#     Adaptive ridge: does not run fast\n",
    "    \n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "    X, Z, y = get_X_Z_y(X, y, T)\n",
    "\n",
    "    N, P = size(X)\n",
    "    # Add variables\n",
    "\n",
    "    @variable(model, β0[j=1:P])\n",
    "    @variable(model, V0[j=1:P, k=1:T*P+T])\n",
    "    @variable(model, t>=0)\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Min, t + ρ_β0 * sum(β0[j]^2 for j=1:P)\n",
    "                            + ρ_V0 * sum(V0[j,k]^2 for j=1:P for k=1:T*P+T)\n",
    "    )\n",
    "\n",
    "    @constraint(model, t >= 1 / N * sum((y[i] - sum(X[i, j] * (β0[j]\n",
    "                 + sum(V0[j, l] * Z[i, l] for l=1:(T * P + T))\n",
    "                 )\n",
    "                for j=1:P))^2\n",
    "        for i=1:N))\n",
    "\n",
    "    optimize!(model);\n",
    "    return objective_value(model), getvalue.(β0), getvalue.(V0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "cb71aa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_X_Z_y (generic function with 1 method)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_X_Z_y(X, y, T)\n",
    "    \"\"\"\n",
    "    Input: training data X and corresponding labels y ; how many time-steps from the past to be used\n",
    "    Output: the past features X with past targets y as a Z training data (no present features)\n",
    "    \"\"\"\n",
    "    n, p = size(X)\n",
    "    #T past time steps * p features + T targets\n",
    "    Z = ones(n-T, T*p+T)\n",
    "    for i=T+1:n\n",
    "        for t=1:T\n",
    "            Z[i-T,1+p*(t-1):p*t] = X[i-t,:]\n",
    "        end\n",
    "        Z[i-T, (p*T+1):end] = y[i-T:i-1]\n",
    "    end\n",
    "    return X[T+1:end,:], Z, y[T+1:end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "6c2e09a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.3188635357368816 1.0301917094157778; 1.3699081609434685 1.1463534063406784], [0.6884037525746151 0.5927909913588593 … -0.2486410898872586 0.43491462796847274; 1.3188635357368816 1.0301917094157778 … 0.43491462796847274 0.7909332310183328], [0.7909332310183328, 0.7909332310183328])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e, Z, f = get_X_Z_y(X[1:5,1:2], y[1:5], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "3b08d166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02445443166082395, [0.18720284895700318, 0.13539976712246812, 0.10930884939275816], [0.018415765261267765 0.006113858247772819 … -0.02126077974487629 0.005548432581699949; 0.04180042388132868 0.025110266568299063 … -0.008627657122849172 0.026815264905921357; 0.04353282905846895 0.02966493372503107 … 0.015643241897700687 0.03394630338098259])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, β0, V0 = adaptive_ridge_regression_standard(X[1:100,1:3], y[1:100], 0.1, 0.1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "002f9b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×12 Matrix{Float64}:\n",
       " 0.0184158  0.00611386  -0.014725   …  0.0141299  -0.0212608   0.00554843\n",
       " 0.0418004  0.0251103    0.0143688     0.0276159  -0.00862766  0.0268153\n",
       " 0.0435328  0.0296649    0.0211816     0.0305751   0.0156432   0.0339463"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "a313920b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_adaptive (generic function with 2 methods)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluate_adaptive(X, y, y_true, β0, V0, T)\n",
    "    X, Z, y = get_X_Z_y(X, y, T)\n",
    "    \n",
    "    N, P = size(X)\n",
    "\n",
    "    pred = [sum(X[i, j] * (β0[j] + sum(V0[j, :] .* Z[i, :])) for j=1:P) for i=1:N]\n",
    "    if size(y_true) == size(pred)\n",
    "        err = [abs(y_true[i]-pred[i]) for i=1:N]\n",
    "    else\n",
    "        y_true = y_true[T+1:end]\n",
    "        err = [abs(y_true[i]-pred[i]) for i=1:N]\n",
    "    end\n",
    "    return err\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "bbb311d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_adaptive_retrained (generic function with 1 method)"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluate_adaptive_retrained(X, y, y_true, β0_list, V0_list, T)\n",
    "    X, Z, y = get_X_Z_y(X, y, T)\n",
    "    \n",
    "    N, P = size(X)\n",
    "\n",
    "    pred = [sum(X[i, j] * (β0_list[i,j] + sum(V0_list[i, j, :] .* Z[i, :])) for j=1:P) for i=1:N]\n",
    "    if size(y_true) == size(pred)\n",
    "        err = [abs(y_true[i]-pred[i]) for i=1:N]\n",
    "    else\n",
    "        y_true = y_true[T+1:end]\n",
    "        err = [abs(y_true[i]-pred[i]) for i=1:N]\n",
    "    end\n",
    "    return err\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "1a0ebb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2772791481270517, -0.22139104249186187, 0.25353125721465, 0.13052228762534424, 0.16184232903639695, 0.02133275894336177, -0.23829147212091226, 0.14926772611852196, 0.28826028113700647, 0.7460128800839689  …  0.6974694989745849, 0.4334507651952026, 0.4535167025297599, 0.4228111533412082, -0.9470370564072363, -0.004111494312688896, 0.12274140243903106, 0.08108513878897414, -1.4025705199739362, 0.08251350395763898], [0.7111257347162706, 0.768332857553357, 0.1994216466416816, 0.6812973681052467, 0.08794262534614797, 0.24054297764516547, 0.02174221306394139, 0.4729100683816176, 0.04127284032870793, 0.5794229469798875  …  0.29188476681020137, 0.3992553152476475, 0.02696830662909544, 0.5024620796474668, 0.7050416471102421, 0.23408638321844036, 0.4254973199898675, 0.7075806393798176, 0.9146849288371715, 0.6178682421677187])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, err = evaluate_adaptive(X[1:100,1:3], y[1:100], y[4:100], β0, V0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "8059e02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval_method (generic function with 3 methods)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function eval_method(X, y, y_true, split_, past, num_past, val, uncertainty, ϵ_inf, δ_inf, last_yT,\n",
    "        ϵ_l2, δ_l2, ρ, reg, max_cuts, verbose,\n",
    "        fix_β0, more_data_for_β0, benders, ridge, linear_adapt)\n",
    "\n",
    "    threshold_benders = 0.01\n",
    "    n, p = size(X)\n",
    "    split_index = floor(Int,n*split_)\n",
    "    #TODO change spli_index with max(split inex, 1)\n",
    "    X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, split_index-num_past*past+1, num_past*past, val, uncertainty, last_yT)\n",
    "\n",
    "    β_list0 = zeros(val, p)\n",
    "    β_listt = zeros(val, p)\n",
    "    β_listl2 = zeros(val, p)\n",
    "    β_l2_init = l2_regression(X0,y0,ρ);\n",
    "    β0_list_linear_adapt = zeros(val, p)\n",
    "    V0_list_linear_adapt = zeros(val, p, (past-1)*p+past-1)\n",
    "    err = ones(val)\n",
    "    \n",
    "    ###Linear decision rule\n",
    "    _, β0_0, V0_0 = adaptive_ridge_regression_standard(X0, y0, ρ, ρ, past-1)\n",
    "    print(size(V0))\n",
    "    \n",
    "    for s=1:val\n",
    "        #TODO Check how much I use from the past\n",
    "        if more_data_for_β0\n",
    "            X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, split_index-num_past*past+1, s+(num_past-1)*past, past-1, uncertainty, last_yT)\n",
    "        else\n",
    "            X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, s+split_index-num_past*past+1, (num_past-1)*past, past-1, uncertainty, last_yT)\n",
    "        end\n",
    "\n",
    "        if benders\n",
    "            ##TODO handle fix_beta0\n",
    "            obj, β0_val, α, y_val = master_problem(X0, Xt, y0, D_min, D_max, threshold_benders, ϵ_inf, δ_inf, reg, ρ, max_cuts, verbose)\n",
    "            _, βt_val = S_primal(Xt, y_val, β0_val, ϵ_inf, δ_inf);\n",
    "        else\n",
    "            if ridge\n",
    "                obj, βt_val, β0_val = master_primal_l2_ridge(X0, Xt, y0, D_min, D_max, ϵ_inf, δ_inf, reg, ρ, ϵ_l2, δ_l2, fix_β0, β_l2_init)\n",
    "            else\n",
    "                obj, βt_val, β0_val = master_primal_l2(X0, Xt, y0, D_min, D_max, ϵ_inf, δ_inf, reg, ρ, ϵ_l2, δ_l2, fix_β0, β_l2_init)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if linear_adapt\n",
    "            #err[s] = evaluate_adaptive(vcat(X0[end-past+2:end,:],Xt), vcat(y0[end-past+2:end,:],yt), yt_true, β0, V0, past)\n",
    "            _, β0, V0 = adaptive_ridge_regression_standard(vcat(X0,Xt), vcat(y0,yt), ρ, ρ, past-1)\n",
    "            β0_list_linear_adapt[s,:] = β0\n",
    "            V0_list_linear_adapt[s,:,:] = V0\n",
    "            N = size(X0)[1]\n",
    "        end\n",
    "        \n",
    "        β_listt[s,:] = βt_val[past-1,:]\n",
    "        β_list0[s,:] = β0_val\n",
    "        β_l2 = l2_regression(vcat(X0,Xt),vcat(y0,yt),ρ);\n",
    "        β_listl2[s,:] = β_l2\n",
    "\n",
    "    end\n",
    "\n",
    "    X0, y0, Xt, yt, _, D_min, D_max = prepare_data_from_y(X, y, 1, split_index, val, uncertainty, last_yT)\n",
    "    _, _, _, _, yt_true, _, _ = prepare_data_from_y(X, y_true, 1, split_index, val, uncertainty, last_yT)\n",
    "\n",
    "    N = size(X0)[1]\n",
    "    err_linear_rule = evaluate_adaptive(vcat(X0[N-past+2:end,:],Xt), vcat(y0[N-past+2:end,:],yt), yt_true, β0_0, V0_0, past-1)\n",
    "    err_linear_rule_retrained = evaluate_adaptive_retrained(vcat(X0[N-past+2:end,:],Xt), vcat(y0[N-past+2:end,:],yt), \n",
    "        yt_true, β0_list_linear_adapt, V0_list_linear_adapt, past-1)\n",
    "    \n",
    "    err_0 = [abs(yt_true[s]-dot(Xt[s,:],β_list0[s,:])) for s=1:val]\n",
    "    err_t = [abs(yt_true[s]-dot(Xt[s,:],β_listt[s,:])) for s=1:val]\n",
    "    err_baseline = [abs(yt_true[s]-dot(Xt[s,:],β_l2_init)) for s=1:val]\n",
    "    err_l2 = [abs(yt_true[s]-dot(Xt[s,:],β_listl2[s,:])) for s=1:val]\n",
    "\n",
    "    println(\"\\n### β0 Baseline ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_baseline))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_baseline, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_baseline, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_baseline, yt_true))\n",
    "\n",
    "    println(\"\\n### β0 Baseline Retrained ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_l2))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_l2, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_l2, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_l2, yt_true))\n",
    "    \n",
    "    println(\"\\n### β0 V0 Linear rule Adaptive ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_linear_rule))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_linear_rule, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_linear_rule, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_linear_rule, yt_true))\n",
    "    \n",
    "    println(\"\\n### β0 V0 Linear rule Adaptive Retrained ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_linear_rule_retrained))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_linear_rule_retrained, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_linear_rule_retrained, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_linear_rule_retrained, yt_true))\n",
    "\n",
    "    println(\"\\n### β0 Adaptive ###\")\n",
    "    println(\"MAE 0: \", mean(err_0))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_0, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_0, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_0, yt_true))\n",
    "\n",
    "    println(\"\\n### βt Adaptive ###\")\n",
    "    println(\"MAE t: \", mean(err_t))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_t, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_t, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_t, yt_true))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "f1f231c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12)\n",
      "### β0 Baseline ###\n",
      "MAE Baseline: 0.36407079601439907\n",
      "CVAR 0.05 :0.8510795192177238\n",
      "CVAR 0.15 :0.7466824544592758\n",
      "R2 : -0.1550215017223442\n",
      "\n",
      "### β0 Baseline Retrained ###\n",
      "MAE Baseline: 0.323850200854258\n",
      "CVAR 0.05 :0.8483059374560837\n",
      "CVAR 0.15 :0.6673628477345281\n",
      "R2 : 0.08364335314613436\n",
      "\n",
      "### β0 V0 Linear rule Adaptive ###\n",
      "MAE Baseline: 0.4537841583788847\n",
      "CVAR 0.05 :0.7043645703192598\n",
      "CVAR 0.15 :0.6707402082410436\n",
      "R2 : -0.4229831005260869\n",
      "\n",
      "### β0 V0 Linear rule Adaptive Retrained ###\n",
      "MAE Baseline: 0.3161653386943477\n",
      "CVAR 0.05 :0.6351686862864883\n",
      "CVAR 0.15 :0.626427302829083\n",
      "R2 : 0.20223588899848288\n",
      "\n",
      "### β0 Adaptive ###\n",
      "MAE 0: 0.32637771499749\n",
      "CVAR 0.05 :0.840372966556379\n",
      "CVAR 0.15 :0.6761183816692053\n",
      "R2 : 0.0663055424040575\n",
      "\n",
      "### βt Adaptive ###\n",
      "MAE t: 0.3197312140562855\n",
      "CVAR 0.05 :0.8162101269545656\n",
      "CVAR 0.15 :0.6228385217610699\n",
      "R2 : 0.14510811464986773\n"
     ]
    }
   ],
   "source": [
    "linear_adapt = true\n",
    "split_ = N0/(N0+Nt)\n",
    "past = 5\n",
    "num_past = floor(Int, Nt/past)\n",
    "\n",
    "last_yT = false\n",
    "max_cuts = 10\n",
    "verbose = false\n",
    "\n",
    "uncertainty = 0.#1\n",
    "δ_inf = 0.02\n",
    "ϵ_inf = 0.02\n",
    "\n",
    "ϵ_l2 = 0.05\n",
    "δ_l2 = 0.05\n",
    "\n",
    "reg = 1/(past*num_past)\n",
    "ρ = 0.1\n",
    "val = Nt-1; #n-split_index;\n",
    "\n",
    "fix_β0 = false\n",
    "more_data_for_β0 = false\n",
    "benders = false\n",
    "ridge = true\n",
    "\n",
    "eval_method(X, y, y_true, split_, past, num_past, val, uncertainty, ϵ_inf, δ_inf, last_yT, \n",
    "        ϵ_l2, δ_l2, ρ, reg, max_cuts, verbose, \n",
    "        fix_β0, more_data_for_β0, benders, ridge, linear_adapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8f815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
