{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "675324d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic license - for non-commercial use only - expires 2022-09-11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gurobi.Env(Ptr{Nothing} @0x00007fb145b0c800, false, 0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV, DataFrames, Statistics, StatsBase, Distributions, JuMP, Gurobi, LinearAlgebra\n",
    "\n",
    "const GRB_ENV = Gurobi.Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4fd85013",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SAFI #####\n",
    "\n",
    "X_test_adaptive = CSV.read(\"data/X_test_adaptive.csv\", DataFrame)\n",
    "y_test = CSV.read(\"data/y_test_speed.csv\", DataFrame)\n",
    "# select!(X_test_adaptive, Not([:RANSACRegressor, :GaussianProcessRegressor, :KernelRidge, :Lars, :AdaBoostRegressor,\n",
    "#                      :DummyRegressor, :ExtraTreeRegressor, :Lasso, :LassoLars, :PassiveAggressiveRegressor]))\n",
    "\n",
    "X = X_test_adaptive#[!,1]\n",
    "#X = X[!,[2, 3,5,8,9,11,12,13,25]]\n",
    "y = y_test[!, \"speed\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f819715",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = size(y)[1]\n",
    "y_safi = (y .- mean(y[1:floor(Int, n/2),:], dims =1))./ std(y[1:floor(Int, n/2),:], dims=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b3f7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Int64</th><th>DataType</th></tr></thead><tbody><p>6 rows × 7 columns</p><tr><th>1</th><td>NUMTECH_speed</td><td>0.0329076</td><td>-4.63357</td><td>0.0733019</td><td>5.13105</td><td>0</td><td>Float64</td></tr><tr><th>2</th><td>speed</td><td>0.0566938</td><td>-5.07005</td><td>0.104789</td><td>3.51963</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>DT_speed</td><td>0.0672517</td><td>-5.52488</td><td>0.106152</td><td>5.08333</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>TABNET_speed</td><td>0.0081165</td><td>-4.80957</td><td>0.0602244</td><td>3.33884</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>RIDGE_speed</td><td>0.0248569</td><td>-5.41145</td><td>0.114219</td><td>3.64899</td><td>0</td><td>Float64</td></tr><tr><th>6</th><td>LASSO_speed</td><td>0.0176461</td><td>-4.4157</td><td>0.117052</td><td>3.32267</td><td>0</td><td>Float64</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64 & Float64 & Float64 & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & NUMTECH\\_speed & 0.0329076 & -4.63357 & 0.0733019 & 5.13105 & 0 & Float64 \\\\\n",
       "\t2 & speed & 0.0566938 & -5.07005 & 0.104789 & 3.51963 & 0 & Float64 \\\\\n",
       "\t3 & DT\\_speed & 0.0672517 & -5.52488 & 0.106152 & 5.08333 & 0 & Float64 \\\\\n",
       "\t4 & TABNET\\_speed & 0.0081165 & -4.80957 & 0.0602244 & 3.33884 & 0 & Float64 \\\\\n",
       "\t5 & RIDGE\\_speed & 0.0248569 & -5.41145 & 0.114219 & 3.64899 & 0 & Float64 \\\\\n",
       "\t6 & LASSO\\_speed & 0.0176461 & -4.4157 & 0.117052 & 3.32267 & 0 & Float64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m6×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable      \u001b[0m\u001b[1m mean      \u001b[0m\u001b[1m min      \u001b[0m\u001b[1m median    \u001b[0m\u001b[1m max     \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltyp\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol        \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataT\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ NUMTECH_speed  0.0329076  -4.63357  0.0733019  5.13105         0  Float ⋯\n",
       "   2 │ speed          0.0566938  -5.07005  0.104789   3.51963         0  Float\n",
       "   3 │ DT_speed       0.0672517  -5.52488  0.106152   5.08333         0  Float\n",
       "   4 │ TABNET_speed   0.0081165  -4.80957  0.0602244  3.33884         0  Float\n",
       "   5 │ RIDGE_speed    0.0248569  -5.41145  0.114219   3.64899         0  Float ⋯\n",
       "   6 │ LASSO_speed    0.0176461  -4.4157   0.117052   3.32267         0  Float\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(X_test_adaptive[:,2:7] .- y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c87c03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>std</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>6 rows × 3 columns</p><tr><th>1</th><td>NUMTECH_speed</td><td>0.0329076</td><td>1.26841</td></tr><tr><th>2</th><td>speed</td><td>0.0566938</td><td>0.877551</td></tr><tr><th>3</th><td>DT_speed</td><td>0.0672517</td><td>1.02984</td></tr><tr><th>4</th><td>TABNET_speed</td><td>0.0081165</td><td>0.905541</td></tr><tr><th>5</th><td>RIDGE_speed</td><td>0.0248569</td><td>0.999924</td></tr><tr><th>6</th><td>LASSO_speed</td><td>0.0176461</td><td>0.978456</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& variable & mean & std\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & NUMTECH\\_speed & 0.0329076 & 1.26841 \\\\\n",
       "\t2 & speed & 0.0566938 & 0.877551 \\\\\n",
       "\t3 & DT\\_speed & 0.0672517 & 1.02984 \\\\\n",
       "\t4 & TABNET\\_speed & 0.0081165 & 0.905541 \\\\\n",
       "\t5 & RIDGE\\_speed & 0.0248569 & 0.999924 \\\\\n",
       "\t6 & LASSO\\_speed & 0.0176461 & 0.978456 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m6×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable      \u001b[0m\u001b[1m mean      \u001b[0m\u001b[1m std      \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol        \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64  \u001b[0m\n",
       "─────┼────────────────────────────────────\n",
       "   1 │ NUMTECH_speed  0.0329076  1.26841\n",
       "   2 │ speed          0.0566938  0.877551\n",
       "   3 │ DT_speed       0.0672517  1.02984\n",
       "   4 │ TABNET_speed   0.0081165  0.905541\n",
       "   5 │ RIDGE_speed    0.0248569  0.999924\n",
       "   6 │ LASSO_speed    0.0176461  0.978456"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(X_test_adaptive[:,2:7] .- y, :mean, :std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42619d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>std</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>24 rows × 3 columns</p><tr><th>1</th><td>BayesianRidge</td><td>3.01734</td><td>59.6378</td></tr><tr><th>2</th><td>ElasticNet</td><td>1.44953</td><td>64.0703</td></tr><tr><th>3</th><td>ElasticNetCV</td><td>0.931037</td><td>60.2554</td></tr><tr><th>4</th><td>GammaRegressor</td><td>0.057711</td><td>68.6374</td></tr><tr><th>5</th><td>GeneralizedLinearRegressor</td><td>0.582401</td><td>67.0447</td></tr><tr><th>6</th><td>HistGradientBoostingRegressor</td><td>20.8497</td><td>65.9726</td></tr><tr><th>7</th><td>HuberRegressor</td><td>-5.2734</td><td>60.1917</td></tr><tr><th>8</th><td>LarsCV</td><td>1.13659</td><td>59.9771</td></tr><tr><th>9</th><td>Lasso</td><td>1.4169</td><td>59.9211</td></tr><tr><th>10</th><td>LassoCV</td><td>-0.0565766</td><td>59.6822</td></tr><tr><th>11</th><td>LassoLarsCV</td><td>0.0127972</td><td>59.6895</td></tr><tr><th>12</th><td>LassoLarsIC</td><td>-0.0599784</td><td>59.5546</td></tr><tr><th>13</th><td>LinearRegression</td><td>4.21631</td><td>60.2873</td></tr><tr><th>14</th><td>LinearSVR</td><td>-6.30994</td><td>60.2945</td></tr><tr><th>15</th><td>MLPRegressor</td><td>2.65384</td><td>69.6359</td></tr><tr><th>16</th><td>OrthogonalMatchingPursuit</td><td>-4.25956</td><td>59.4915</td></tr><tr><th>17</th><td>OrthogonalMatchingPursuitCV</td><td>-0.633744</td><td>60.0039</td></tr><tr><th>18</th><td>PoissonRegressor</td><td>4.08991</td><td>69.0245</td></tr><tr><th>19</th><td>Ridge</td><td>3.84331</td><td>59.9261</td></tr><tr><th>20</th><td>RidgeCV</td><td>3.21162</td><td>59.7046</td></tr><tr><th>21</th><td>SGDRegressor</td><td>14.5833</td><td>60.9567</td></tr><tr><th>22</th><td>TransformedTargetRegressor</td><td>4.21631</td><td>60.2873</td></tr><tr><th>23</th><td>TweedieRegressor</td><td>0.582401</td><td>67.0447</td></tr><tr><th>24</th><td>LGBMRegressor</td><td>14.6963</td><td>64.1774</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& variable & mean & std\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & BayesianRidge & 3.01734 & 59.6378 \\\\\n",
       "\t2 & ElasticNet & 1.44953 & 64.0703 \\\\\n",
       "\t3 & ElasticNetCV & 0.931037 & 60.2554 \\\\\n",
       "\t4 & GammaRegressor & 0.057711 & 68.6374 \\\\\n",
       "\t5 & GeneralizedLinearRegressor & 0.582401 & 67.0447 \\\\\n",
       "\t6 & HistGradientBoostingRegressor & 20.8497 & 65.9726 \\\\\n",
       "\t7 & HuberRegressor & -5.2734 & 60.1917 \\\\\n",
       "\t8 & LarsCV & 1.13659 & 59.9771 \\\\\n",
       "\t9 & Lasso & 1.4169 & 59.9211 \\\\\n",
       "\t10 & LassoCV & -0.0565766 & 59.6822 \\\\\n",
       "\t11 & LassoLarsCV & 0.0127972 & 59.6895 \\\\\n",
       "\t12 & LassoLarsIC & -0.0599784 & 59.5546 \\\\\n",
       "\t13 & LinearRegression & 4.21631 & 60.2873 \\\\\n",
       "\t14 & LinearSVR & -6.30994 & 60.2945 \\\\\n",
       "\t15 & MLPRegressor & 2.65384 & 69.6359 \\\\\n",
       "\t16 & OrthogonalMatchingPursuit & -4.25956 & 59.4915 \\\\\n",
       "\t17 & OrthogonalMatchingPursuitCV & -0.633744 & 60.0039 \\\\\n",
       "\t18 & PoissonRegressor & 4.08991 & 69.0245 \\\\\n",
       "\t19 & Ridge & 3.84331 & 59.9261 \\\\\n",
       "\t20 & RidgeCV & 3.21162 & 59.7046 \\\\\n",
       "\t21 & SGDRegressor & 14.5833 & 60.9567 \\\\\n",
       "\t22 & TransformedTargetRegressor & 4.21631 & 60.2873 \\\\\n",
       "\t23 & TweedieRegressor & 0.582401 & 67.0447 \\\\\n",
       "\t24 & LGBMRegressor & 14.6963 & 64.1774 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m24×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable                      \u001b[0m\u001b[1m mean       \u001b[0m\u001b[1m std     \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol                        \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────\n",
       "   1 │ BayesianRidge                   3.01734    59.6378\n",
       "   2 │ ElasticNet                      1.44953    64.0703\n",
       "   3 │ ElasticNetCV                    0.931037   60.2554\n",
       "   4 │ GammaRegressor                  0.057711   68.6374\n",
       "   5 │ GeneralizedLinearRegressor      0.582401   67.0447\n",
       "   6 │ HistGradientBoostingRegressor  20.8497     65.9726\n",
       "   7 │ HuberRegressor                 -5.2734     60.1917\n",
       "   8 │ LarsCV                          1.13659    59.9771\n",
       "   9 │ Lasso                           1.4169     59.9211\n",
       "  10 │ LassoCV                        -0.0565766  59.6822\n",
       "  11 │ LassoLarsCV                     0.0127972  59.6895\n",
       "  ⋮  │               ⋮                    ⋮          ⋮\n",
       "  15 │ MLPRegressor                    2.65384    69.6359\n",
       "  16 │ OrthogonalMatchingPursuit      -4.25956    59.4915\n",
       "  17 │ OrthogonalMatchingPursuitCV    -0.633744   60.0039\n",
       "  18 │ PoissonRegressor                4.08991    69.0245\n",
       "  19 │ Ridge                           3.84331    59.9261\n",
       "  20 │ RidgeCV                         3.21162    59.7046\n",
       "  21 │ SGDRegressor                   14.5833     60.9567\n",
       "  22 │ TransformedTargetRegressor      4.21631    60.2873\n",
       "  23 │ TweedieRegressor                0.582401   67.0447\n",
       "  24 │ LGBMRegressor                  14.6963     64.1774\n",
       "\u001b[36m                                            3 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### ENERGY #####\n",
    "X_energy = CSV.read(\"data/energy_predictions_test_val.csv\", DataFrame)\n",
    "y_energy = CSV.read(\"data/energy_y_test_val.csv\", DataFrame, header = 0);\n",
    "describe(X_energy[4000:8000,2:25] .- y_energy[4000:8000,1], :mean, :std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1209af45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>std</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>24 rows × 3 columns</p><tr><th>1</th><td>BayesianRidge</td><td>28.553</td><td>52.4433</td></tr><tr><th>2</th><td>ElasticNet</td><td>30.3495</td><td>56.4427</td></tr><tr><th>3</th><td>ElasticNetCV</td><td>27.5846</td><td>53.5768</td></tr><tr><th>4</th><td>GammaRegressor</td><td>32.5871</td><td>60.4062</td></tr><tr><th>5</th><td>GeneralizedLinearRegressor</td><td>32.3298</td><td>58.7354</td></tr><tr><th>6</th><td>HistGradientBoostingRegressor</td><td>41.7822</td><td>55.1454</td></tr><tr><th>7</th><td>HuberRegressor</td><td>24.0737</td><td>55.4182</td></tr><tr><th>8</th><td>LarsCV</td><td>26.6521</td><td>53.7404</td></tr><tr><th>9</th><td>Lasso</td><td>26.787</td><td>53.6174</td></tr><tr><th>10</th><td>LassoCV</td><td>26.7808</td><td>53.3346</td></tr><tr><th>11</th><td>LassoLarsCV</td><td>26.7963</td><td>53.335</td></tr><tr><th>12</th><td>LassoLarsIC</td><td>26.8104</td><td>53.1768</td></tr><tr><th>13</th><td>LinearRegression</td><td>30.1099</td><td>52.3975</td></tr><tr><th>14</th><td>LinearSVR</td><td>23.871</td><td>55.7251</td></tr><tr><th>15</th><td>MLPRegressor</td><td>43.1145</td><td>54.7437</td></tr><tr><th>16</th><td>OrthogonalMatchingPursuit</td><td>26.2301</td><td>53.5649</td></tr><tr><th>17</th><td>OrthogonalMatchingPursuitCV</td><td>26.6696</td><td>53.7534</td></tr><tr><th>18</th><td>PoissonRegressor</td><td>32.591</td><td>60.9809</td></tr><tr><th>19</th><td>Ridge</td><td>29.5544</td><td>52.2707</td></tr><tr><th>20</th><td>RidgeCV</td><td>28.8434</td><td>52.3718</td></tr><tr><th>21</th><td>SGDRegressor</td><td>35.7161</td><td>51.5023</td></tr><tr><th>22</th><td>TransformedTargetRegressor</td><td>30.1099</td><td>52.3975</td></tr><tr><th>23</th><td>TweedieRegressor</td><td>32.3298</td><td>58.7354</td></tr><tr><th>24</th><td>LGBMRegressor</td><td>36.5706</td><td>54.7452</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& variable & mean & std\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & BayesianRidge & 28.553 & 52.4433 \\\\\n",
       "\t2 & ElasticNet & 30.3495 & 56.4427 \\\\\n",
       "\t3 & ElasticNetCV & 27.5846 & 53.5768 \\\\\n",
       "\t4 & GammaRegressor & 32.5871 & 60.4062 \\\\\n",
       "\t5 & GeneralizedLinearRegressor & 32.3298 & 58.7354 \\\\\n",
       "\t6 & HistGradientBoostingRegressor & 41.7822 & 55.1454 \\\\\n",
       "\t7 & HuberRegressor & 24.0737 & 55.4182 \\\\\n",
       "\t8 & LarsCV & 26.6521 & 53.7404 \\\\\n",
       "\t9 & Lasso & 26.787 & 53.6174 \\\\\n",
       "\t10 & LassoCV & 26.7808 & 53.3346 \\\\\n",
       "\t11 & LassoLarsCV & 26.7963 & 53.335 \\\\\n",
       "\t12 & LassoLarsIC & 26.8104 & 53.1768 \\\\\n",
       "\t13 & LinearRegression & 30.1099 & 52.3975 \\\\\n",
       "\t14 & LinearSVR & 23.871 & 55.7251 \\\\\n",
       "\t15 & MLPRegressor & 43.1145 & 54.7437 \\\\\n",
       "\t16 & OrthogonalMatchingPursuit & 26.2301 & 53.5649 \\\\\n",
       "\t17 & OrthogonalMatchingPursuitCV & 26.6696 & 53.7534 \\\\\n",
       "\t18 & PoissonRegressor & 32.591 & 60.9809 \\\\\n",
       "\t19 & Ridge & 29.5544 & 52.2707 \\\\\n",
       "\t20 & RidgeCV & 28.8434 & 52.3718 \\\\\n",
       "\t21 & SGDRegressor & 35.7161 & 51.5023 \\\\\n",
       "\t22 & TransformedTargetRegressor & 30.1099 & 52.3975 \\\\\n",
       "\t23 & TweedieRegressor & 32.3298 & 58.7354 \\\\\n",
       "\t24 & LGBMRegressor & 36.5706 & 54.7452 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m24×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable                      \u001b[0m\u001b[1m mean    \u001b[0m\u001b[1m std     \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol                        \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼─────────────────────────────────────────────────\n",
       "   1 │ BayesianRidge                  28.553   52.4433\n",
       "   2 │ ElasticNet                     30.3495  56.4427\n",
       "   3 │ ElasticNetCV                   27.5846  53.5768\n",
       "   4 │ GammaRegressor                 32.5871  60.4062\n",
       "   5 │ GeneralizedLinearRegressor     32.3298  58.7354\n",
       "   6 │ HistGradientBoostingRegressor  41.7822  55.1454\n",
       "   7 │ HuberRegressor                 24.0737  55.4182\n",
       "   8 │ LarsCV                         26.6521  53.7404\n",
       "   9 │ Lasso                          26.787   53.6174\n",
       "  10 │ LassoCV                        26.7808  53.3346\n",
       "  11 │ LassoLarsCV                    26.7963  53.335\n",
       "  ⋮  │               ⋮                   ⋮        ⋮\n",
       "  15 │ MLPRegressor                   43.1145  54.7437\n",
       "  16 │ OrthogonalMatchingPursuit      26.2301  53.5649\n",
       "  17 │ OrthogonalMatchingPursuitCV    26.6696  53.7534\n",
       "  18 │ PoissonRegressor               32.591   60.9809\n",
       "  19 │ Ridge                          29.5544  52.2707\n",
       "  20 │ RidgeCV                        28.8434  52.3718\n",
       "  21 │ SGDRegressor                   35.7161  51.5023\n",
       "  22 │ TransformedTargetRegressor     30.1099  52.3975\n",
       "  23 │ TweedieRegressor               32.3298  58.7354\n",
       "  24 │ LGBMRegressor                  36.5706  54.7452\n",
       "\u001b[36m                                         3 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(abs.(X_energy[4000:8000,2:25] .- y_energy[4000:8000,1]), :mean, :std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c7b9dfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8494, 1)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(y_safi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bc758d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 900\n",
    "Nt = 100\n",
    "p = 10\n",
    "\n",
    "δ = 0.5\n",
    "δ_pert = 0.5\n",
    "σ_pert = 1\n",
    "#y = randn(N0+Nt)\n",
    "n0 = floor(Int, size(y_safi)[1]/2)\n",
    "y_true = y_safi[n0-N0:n0+Nt-1]\n",
    "y = copy(y_true)\n",
    "d = Uniform(-δ, δ)\n",
    "biases = rand(d, p)\n",
    "variances = rand(p)\n",
    "gradual_after = false\n",
    "gradual_before = true\n",
    "\n",
    "#Create the features X by taking y and adding noise.\n",
    "X = zeros(N0+Nt, p)\n",
    "\n",
    "for i=1:p\n",
    "    d = Normal(biases[i], variances[i])\n",
    "    X[:,i] = y.+ rand(d, N0+Nt)\n",
    "end\n",
    "\n",
    "#### Perturbations on base learners because of data drift\n",
    "d = Normal(0, δ_pert)\n",
    "biases_perturb = rand(d, p)\n",
    "d = Uniform(0, σ_pert)\n",
    "variances_perturb = rand(d, p)\n",
    "\n",
    "if gradual_after\n",
    "    for i=1:p\n",
    "        d = Normal(biases_perturb[i], variances_perturb[i])\n",
    "        #We add the perturbation for each model \n",
    "        #We make the perturbations more and more intense across time\n",
    "        X[N0+1:N0+Nt,i] = X[N0+1:N0+Nt,i].+rand(d, Nt).*[t/Nt for t=1:Nt]\n",
    "    end\n",
    "end\n",
    "\n",
    "if gradual_before\n",
    "    for i=1:p\n",
    "        for t=1:Nt\n",
    "            #We compute the perturbation for each model \n",
    "            #We make the perturbations more and more intense across time\n",
    "            d = Normal(t/Nt*δ_pert, t/Nt*σ_pert)\n",
    "            X[N0+t,i] = X[N0+t,i]+rand(d)\n",
    "        end\n",
    "    end\n",
    "else\n",
    "    for i=1:p\n",
    "        d = Normal(biases_perturb[i], variances_perturb[i])\n",
    "        #We add the perturbation for each model \n",
    "        #We make the perturbations more and more intense across time\n",
    "        X[N0+1:N0+Nt,i] = X[N0+1:N0+Nt,i].+rand(d, Nt)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2f90335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Perturbations on y\n",
    "perturb_y = true\n",
    "perturb_y_norm = true\n",
    "y_bias = -0.1\n",
    "y_var = 0.5\n",
    "\n",
    "if perturb_y\n",
    "    if perturb_y_norm\n",
    "        d = Normal(y_bias, y_var)\n",
    "        y[N0+1:N0+Nt] = y[N0+1:N0+Nt] .+ rand(d, Nt)\n",
    "    else\n",
    "        d = Uniform(-y_bias, y_bias)\n",
    "        y[N0+1:N0+Nt] = y[N0+1:N0+Nt] .+ rand(d, Nt)\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a685535d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: linear_adapt not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: linear_adapt not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[135]:25",
      " [2] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "split_ = N0/(N0+Nt)\n",
    "past = 10\n",
    "num_past = floor(Int, Nt/past)\n",
    "\n",
    "last_yT = false\n",
    "max_cuts = 10\n",
    "verbose = false\n",
    "\n",
    "uncertainty = 0.#1\n",
    "δ_inf = 0.02\n",
    "ϵ_inf = 0.02\n",
    "\n",
    "ϵ_l2 = 0.05\n",
    "δ_l2 = 0.05\n",
    "\n",
    "reg = 1/(past*num_past)\n",
    "ρ = 0.1\n",
    "val = Nt-1; #n-split_index;\n",
    "\n",
    "fix_β0 = false\n",
    "more_data_for_β0 = false\n",
    "benders = false\n",
    "ridge = true\n",
    "\n",
    "eval_method(X, y, y_true, split_, past, num_past, val, uncertainty, ϵ_inf, δ_inf, last_yT, \n",
    "        ϵ_l2, δ_l2, ρ, reg, max_cuts, verbose, \n",
    "        fix_β0, more_data_for_β0, benders, ridge, linear_adapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1d6a6fba",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Result index of attribute MathOptInterface.ObjectiveValue(1) out of bounds. There are currently 0 solution(s) in the model.",
     "output_type": "error",
     "traceback": [
      "Result index of attribute MathOptInterface.ObjectiveValue(1) out of bounds. There are currently 0 solution(s) in the model.",
      "",
      "Stacktrace:",
      "  [1] check_result_index_bounds",
      "    @ /Users/iai/builds/InterpretableAI/SystemImage/SysImgBuilder/.julia/packages/MathOptInterface/YDdD3/src/attributes.jl:139 [inlined]",
      "  [2] get(model::Gurobi.Optimizer, attr::MathOptInterface.ObjectiveValue)",
      "    @ Gurobi ~/.julia/packages/Gurobi/BAtIN/src/MOI_wrapper/MOI_wrapper.jl:3086",
      "  [3] get(b::MathOptInterface.Bridges.LazyBridgeOptimizer{Gurobi.Optimizer}, attr::MathOptInterface.ObjectiveValue)",
      "    @ MathOptInterface.Bridges /Users/iai/builds/InterpretableAI/SystemImage/SysImgBuilder/.julia/packages/MathOptInterface/YDdD3/src/Bridges/bridge_optimizer.jl:913",
      "  [4] get(model::MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.AbstractOptimizer, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.GenericModel{Float64, MathOptInterface.Utilities.ModelFunctionConstraints{Float64}}}}, attr::MathOptInterface.ObjectiveValue)",
      "    @ MathOptInterface.Utilities /Applications/Julia-1.6.app/Contents/Resources/julia/lib/julia/sys.dylib:-1",
      "  [5] _moi_get_result(model::MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.AbstractOptimizer, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.GenericModel{Float64, MathOptInterface.Utilities.ModelFunctionConstraints{Float64}}}}, args::MathOptInterface.ObjectiveValue)",
      "    @ JuMP /Users/iai/builds/InterpretableAI/SystemImage/SysImgBuilder/.julia/packages/JuMP/qhoVb/src/JuMP.jl:848",
      "  [6] get(model::Model, attr::MathOptInterface.ObjectiveValue)",
      "    @ JuMP /Users/iai/builds/InterpretableAI/SystemImage/SysImgBuilder/.julia/packages/JuMP/qhoVb/src/JuMP.jl:861",
      "  [7] objective_value(model::Model; result::Int64)",
      "    @ JuMP /Users/iai/builds/InterpretableAI/SystemImage/SysImgBuilder/.julia/packages/JuMP/qhoVb/src/objective.jl:42",
      "  [8] objective_value",
      "    @ /Users/iai/builds/InterpretableAI/SystemImage/SysImgBuilder/.julia/packages/JuMP/qhoVb/src/objective.jl:42 [inlined]",
      "  [9] master_primal_l2_ridge(X0::Matrix{Float64}, Xt::Matrix{Float64}, y0::Vector{Float64}, Dmin::Vector{Float64}, Dmax::Vector{Float64}, epsilon::Float64, delta::Float64, reg::Float64, ρ::Float64, ϵ_l2::Float64, δ_l2::Float64, β0_fix::Bool, β0_val::Vector{Float64})",
      "    @ Main ./In[10]:114",
      " [10] eval_method(X::Matrix{Float64}, y::Vector{Float64}, y_true::Vector{Float64}, split_::Float64, past::Int64, num_past::Int64, val::Int64, uncertainty::Float64, ϵ_inf::Float64, δ_inf::Float64, last_yT::Bool, ϵ_l2::Float64, δ_l2::Float64, ρ::Float64, reg::Float64, max_cuts::Int64, verbose::Bool, fix_β0::Bool, more_data_for_β0::Bool, benders::Bool, ridge::Bool)",
      "    @ Main ./In[9]:74",
      " [11] top-level scope",
      "    @ In[136]:25",
      " [12] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [13] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "split_ = N0/(N0+Nt)\n",
    "past = 20\n",
    "num_past = floor(Int, Nt/past)\n",
    "\n",
    "last_yT = false\n",
    "max_cuts = 10\n",
    "verbose = false\n",
    "\n",
    "uncertainty = 0.#1\n",
    "δ_inf = 0.02\n",
    "ϵ_inf = 0.02\n",
    "\n",
    "ϵ_l2 = 0.05\n",
    "δ_l2 = 0.05\n",
    "\n",
    "reg = 1/(past*num_past)\n",
    "ρ = 0.1\n",
    "val = Nt-1; #n-split_index;\n",
    "\n",
    "fix_β0 = false\n",
    "more_data_for_β0 = false\n",
    "benders = false\n",
    "ridge = true\n",
    "\n",
    "eval_method(X, y, y_true, split_, past, num_past, val, uncertainty, ϵ_inf, δ_inf, last_yT, \n",
    "        ϵ_l2, δ_l2, ρ, reg, max_cuts, verbose, \n",
    "        fix_β0, more_data_for_β0, benders, ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de22ec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "master_problem (generic function with 8 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_Y(X, t)\n",
    "    \"\n",
    "    Create the vector of data for the dual problem\n",
    "    \"\n",
    "    T, p = size(X)\n",
    "    Y = zeros(1,T*p)\n",
    "    Y[(t-1)*p+1:t*p] = X[t,:]\n",
    "    return Y\n",
    "end\n",
    "\n",
    "function get_Z(X)\n",
    "    T, p = size(X)\n",
    "    Z = zeros(T, T*p)\n",
    "    for t=1:T\n",
    "        Z[t,:] = get_Y(X,t)\n",
    "    end\n",
    "    return Z\n",
    "end\n",
    "\n",
    "function get_A(X, t)\n",
    "    T, p = size(X)\n",
    "    A = zeros(p,T*p)\n",
    "    A[:,(t-1)*p+1:t*p] = 1 * Matrix(I, p, p)\n",
    "    return A\n",
    "end\n",
    "\n",
    "\n",
    "function solve_model_benders(m)\n",
    "    \"Solve the Benders master problem\n",
    "    \"\n",
    "    optimize!(m)\n",
    "    U_OA = objective_value(m)\n",
    "    #println(\"U_OA from solve \", U_OA)\n",
    "    return value.(m[:β]), value.(m[:α]), U_OA\n",
    "end\n",
    "\n",
    "function S_primal(X, y, β0, epsilon, delta)\n",
    "\n",
    "    n, p = size(X)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, β[i=1:n,j=1:p])\n",
    "    @variable(model, b[i=1:n]>=0)\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Min, sum(b[i] for i=1:n))\n",
    "\n",
    "    #@constraint(model,[i=1:n], y .- dot(X,β) .<= b)\n",
    "    #@constraint(model,[i=1:n],-y .+ dot(X,β) .<= b)\n",
    "\n",
    "\n",
    "    @constraint(model, res_plus[i=1:n],  - y[i] + dot(X[i,:],β[i,:]) <= b[i])\n",
    "    @constraint(model, res_minus[i=1:n],  y[i] - dot(X[i,:],β[i,:]) <= b[i])\n",
    "\n",
    "    @constraint(model, diff_plus[i=2:n],   β[i,:] .- β[i-1,:] .<= delta)\n",
    "    @constraint(model, diff_minus[i=2:n], - β[i,:] .+ β[i-1,:] .<= delta)\n",
    "\n",
    "    @constraint(model, diff_0_plus[i=1:n],   β[i,:] .- β0 .<= epsilon)\n",
    "    @constraint(model, diff_0_minus[i=1:n], - β[i,:] .+ β0 .<= epsilon)\n",
    "\n",
    "    optimize!(model);\n",
    "\n",
    "    return objective_value(model), getvalue.(β)\n",
    "end\n",
    "\n",
    "\n",
    "function R(X, D_min, D_max, β0, epsilon, delta)\n",
    "    \"\n",
    "    Full dual problem\n",
    "    \"\n",
    "    T, p = size(X)\n",
    "    Z = get_Z(X)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))#Model(with_optimizer(Gurobi.Optimizer))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "    set_optimizer_attribute(model, \"NonConvex\", 2)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, λ[i=1:2, j=1:T] >= 0)\n",
    "    @variable(model, ν[i=1:2, j=1:T-1, k=1:p]>=0)\n",
    "    @variable(model, μ[i=1:2, j=1:T, k=1:p]>=0)\n",
    "\n",
    "    @variable(model, y[j=1:T])\n",
    "\n",
    "\n",
    "    @constraint(model,[t=1:T], λ[1,:] .+ λ[2,:] .== 1)\n",
    "\n",
    "\n",
    "    @constraint(model, transpose(λ[2,:])*Z-transpose(λ[1,:])*Z\n",
    "                        + sum(transpose(ν[1,t,:])*(get_A(X, t+1).-get_A(X, t)) for t=1:T-1)\n",
    "                        + sum(transpose(ν[2,t,:])*(-get_A(X, t+1).+get_A(X, t)) for t=1:T-1)\n",
    "                        + sum(transpose(μ[1,t,:])*get_A(X,t) for t=1:T)\n",
    "                        - sum(transpose(μ[2,t,:])*get_A(X,t) for t=1:T) .== 0)\n",
    "\n",
    "    #y in uncertainty set\n",
    "    @constraint(model, [1:T], D_min .<= y)\n",
    "    @constraint(model, [1:T], y .<= D_max)\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Max, 2*dot(λ[1,:],y) - sum(y)\n",
    "                            - delta * sum(sum(ν[1,t,i]+ν[2,t,i] for i=1:p) for t=1:T-1)\n",
    "                            - sum(dot(epsilon .+ β0, μ[1,t,:]) for t = 1:T)\n",
    "                            - sum(dot(epsilon .- β0, μ[2,t,:]) for t = 1:T)) #\n",
    "    optimize!(model)\n",
    "    return objective_value(model), getvalue.(y), getvalue.(λ), getvalue.(ν), getvalue.(μ)\n",
    "end\n",
    "\n",
    "function master_problem(X0, Xt, y0, D_min, D_max, threshold = 0.1, epsilon = 0.1, delta = 0.1, reg = 1, ρ = 1, max_cuts = 10, verbose=0)\n",
    "    n, p = size(X0)\n",
    "    T, p = size(Xt)\n",
    "    #Z = get_Z(X0)\n",
    "    L_BD = -10000\n",
    "    U_BD = 10000\n",
    "    cuts = 0\n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))#Model(with_optimizer(Gurobi.Optimizer))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, α)\n",
    "    @variable(model, β[j=1:p])\n",
    "\n",
    "    #Warm start for β\n",
    "\n",
    "    β_val0 = l2_regression(X0, y0, ρ)#Random.rand(p)#\n",
    "    #β_val0 = Random.rand(p)\n",
    "\n",
    "\n",
    "    #Initialization\n",
    "    _, y_val0, λ_val0, ν_val0, μ_val0 = R(Xt, D_min, D_max, β_val0, epsilon, delta)\n",
    "\n",
    "    #First constraint\n",
    "    @constraint(model, α >= 2*dot(λ_val0[1,:],y_val0) - sum(y_val0)\n",
    "                            - delta * sum(sum(ν_val0[1,t,i]+ν_val0[2,t,i] for i=1:p) for t=1:T-1)\n",
    "                            - sum(dot(epsilon .+ β, μ_val0[1,t,:]) for t = 1:T)\n",
    "                            - sum(dot(epsilon .- β, μ_val0[2,t,:]) for t = 1:T))\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Min, 1/n*sum((y0[i]-sum(X0[i,j]*β[j] for j=1:p))^2 for i=1:n) + reg*α + ρ*sum(β[j]^2 for j=1:p))\n",
    "\n",
    "    while cuts < max_cuts && U_BD - L_BD > threshold\n",
    "        if verbose\n",
    "            println(\"Lower: \", L_BD, \" Upper: \", U_BD)\n",
    "        end\n",
    "        cuts += 1\n",
    "\n",
    "        #Solve current Master Problem\n",
    "        β_val, α_val, L_BD = solve_model_benders(model)\n",
    "        U_OA, y_val, λ_val, ν_val, μ_val = R(Xt, D_min, D_max, β_val, epsilon, delta)\n",
    "\n",
    "        U_BD = 1/n*sum((y0[i]-sum(X0[i,j]*β_val[j] for j=1:p))^2 for i=1:n) + reg*U_OA + ρ*sum(β_val[j]^2 for j=1:p)\n",
    "\n",
    "        if U_BD - L_BD > threshold\n",
    "            @constraint(model, α >= 2*dot(λ_val[1,:],y_val) - sum(y_val)\n",
    "                            - delta * sum(sum(ν_val[1,t,i]+ν_val[2,t,i] for i=1:p) for t=1:T-1)\n",
    "                            - sum(dot(epsilon .+ β, μ_val[1,t,:]) for t = 1:T)\n",
    "                            - sum(dot(epsilon .- β, μ_val[2,t,:]) for t = 1:T))\n",
    "            if verbose\n",
    "                println(\"Cut added\")\n",
    "                println(\"y_val: \", y_val)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    optimize!(model)\n",
    "    β_val, α_val, L_BD = solve_model_benders(model)\n",
    "    U_OA, y_val, λ_val, ν_val, μ_val = R(Xt, D_min, D_max, β_val, epsilon, delta)\n",
    "    if verbose\n",
    "        println(\"Final model Obj value: \", objective_value(model))\n",
    "        println(\"Lower: \", L_BD, \" Upper: \", U_BD)\n",
    "        println(\"Final y: \", y_val)\n",
    "    end\n",
    "    return objective_value(model), getvalue.(β), getvalue.(α), y_val#, getvalue.(ν), getvalue.(μ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6507fc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l2_regression (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function R2(y_true, y_test)\n",
    "    SSR = sum(abs2.(y_true.-y_test))\n",
    "    SST = sum(abs2.(y_true.-mean(y_true)))\n",
    "    return 1 - SSR/SST\n",
    "end\n",
    "\n",
    "function R2_err(err, y_true)\n",
    "    SSR = sum(abs2.(err))\n",
    "    SST = sum(abs2.(y_true.-mean(y_true)))\n",
    "    return 1 - SSR/SST\n",
    "end\n",
    "\n",
    "\n",
    "function compute_CVaR(errs, α_risk)\n",
    "#     '''\n",
    "#     Compute the Conditional Value at Risk\n",
    "#     :param X: Either the data matrix X or the errors\n",
    "#     :param y: the target values\n",
    "#     :param alpha: the risk value\n",
    "#     :param beta:\n",
    "#     :param b0:\n",
    "#     :param errs: whether you want to input the data matrix X or the errors\n",
    "#     :return: the model and the CVaR\n",
    "#     '''\n",
    "    n = size(errs)[1]\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, τ)\n",
    "    @variable(model, z[1:n] >= 0)\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Min, sum(z)/(α_risk * n) + τ)\n",
    "\n",
    "    @constraint(model, [1:n], errs .- τ .<= z)\n",
    "\n",
    "    optimize!(model)\n",
    "\n",
    "    return objective_value(model)\n",
    "end\n",
    "\n",
    "\n",
    "function eval_method(X, y, y_true, split_, past, num_past, val, uncertainty, ϵ_inf, δ_inf, last_yT,\n",
    "        ϵ_l2, δ_l2, ρ, reg, max_cuts, verbose,\n",
    "        fix_β0, more_data_for_β0, benders, ridge)\n",
    "\n",
    "    threshold_benders = 0.01\n",
    "    n, p = size(X)\n",
    "    split_index = floor(Int,n*split_)\n",
    "    #TODO change spli_index with max(split inex, 1)\n",
    "    X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, split_index-num_past*past+1, num_past*past, val, uncertainty, last_yT)\n",
    "\n",
    "    β_list0 = zeros(val, p)\n",
    "    β_listt = zeros(val, p)\n",
    "    β_listl2 = zeros(val, p)\n",
    "    β_l2_init = l2_regression(X0,y0,ρ);\n",
    "    for s=1:val\n",
    "\n",
    "        if more_data_for_β0\n",
    "            X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, split_index-num_past*past+1, s+(num_past-1)*past, past-1, uncertainty, last_yT)\n",
    "        else\n",
    "            X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, s+split_index-num_past*past+1, (num_past-1)*past, past-1, uncertainty, last_yT)\n",
    "        end\n",
    "\n",
    "\n",
    "        if benders\n",
    "            ##TODO handle fix_beta0\n",
    "            obj, β0_val, α, y_val = master_problem(X0, Xt, y0, D_min, D_max, threshold_benders, ϵ_inf, δ_inf, reg, ρ, max_cuts, verbose)\n",
    "            _, βt_val = S_primal(Xt, y_val, β0_val, ϵ_inf, δ_inf);\n",
    "        else\n",
    "            if ridge\n",
    "                obj, βt_val, β0_val = master_primal_l2_ridge(X0, Xt, y0, D_min, D_max, ϵ_inf, δ_inf, reg, ρ, ϵ_l2, δ_l2, fix_β0, β_l2_init)\n",
    "            else\n",
    "                obj, βt_val, β0_val = master_primal_l2(X0, Xt, y0, D_min, D_max, ϵ_inf, δ_inf, reg, ρ, ϵ_l2, δ_l2, fix_β0, β_l2_init)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        β_listt[s,:] = βt_val[past-1,:]\n",
    "        β_list0[s,:] = β0_val\n",
    "        β_l2 = l2_regression(X0,y0,ρ);\n",
    "        β_listl2[s,:] = β_l2\n",
    "\n",
    "    end\n",
    "\n",
    "    X0, y0, Xt, yt, _, D_min, D_max = prepare_data_from_y(X, y, 1, split_index, val, uncertainty, last_yT)\n",
    "    _, _, _, _, yt_true, _, _ = prepare_data_from_y(X, y_true, 1, split_index, val, uncertainty, last_yT)\n",
    "\n",
    "    err_0 = [abs(yt_true[s]-dot(Xt[s,:],β_list0[s,:])) for s=1:val]\n",
    "    err_t = [abs(yt_true[s]-dot(Xt[s,:],β_listt[s,:])) for s=1:val]\n",
    "    err_baseline = [abs(yt_true[s]-dot(Xt[s,:],β_l2_init)) for s=1:val]\n",
    "    err_l2 = [abs(yt_true[s]-dot(Xt[s,:],β_listl2[s,:])) for s=1:val]\n",
    "\n",
    "    println(\"\\n### β0 Baseline ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_baseline))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_baseline, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_baseline, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_baseline, yt_true))\n",
    "\n",
    "    println(\"\\n### β0 Baseline Retrained ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_l2))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_l2, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_l2, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_l2, yt_true))\n",
    "\n",
    "    println(\"\\n### β0 Adaptive ###\")\n",
    "    println(\"MAE 0: \", mean(err_0))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_0, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_0, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_0, yt_true))\n",
    "\n",
    "    println(\"\\n### βt Adaptive ###\")\n",
    "    println(\"MAE t: \", mean(err_t))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_t, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_t, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_t, yt_true))\n",
    "end\n",
    "\n",
    "\n",
    "function prepare_data_from_y(X, y, n0, n, m, uncertainty, last_yT = false)\n",
    "\n",
    "    X0 = Matrix(X[n0:n0+n,:])\n",
    "    X0[:,1] = ones(n+1)\n",
    "    y0 = y[n0:n0+n,:][:]\n",
    "\n",
    "    yt_true = y[n0+n+1:n0+n+m,:][:]\n",
    "    Xt = Matrix(X[n0+n+1:n0+n+m,:])\n",
    "    Xt[:,1] = ones(m)\n",
    "    yt = yt_true\n",
    "    if last_yT\n",
    "        yt_true[m] = mean(Xt[m])\n",
    "    end\n",
    "\n",
    "    D_min = yt .- uncertainty.*abs.(yt)\n",
    "    D_max = yt .+ uncertainty.*abs.(yt)\n",
    "\n",
    "    return X0, y0, Xt, yt, yt_true, D_min, D_max\n",
    "end\n",
    "\n",
    "\n",
    "function l2_regression(X, y, rho; solver_output=0)\n",
    "    n,p = size(X)\n",
    "\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", solver_output)\n",
    "    set_optimizer_attribute(model, \"NonConvex\", 2)\n",
    "\n",
    "    @variable(model,beta[j=1:p])\n",
    "    @variable(model, sse>=0)\n",
    "    #@variable(model, reg>=0)\n",
    "    @constraint(model, sum((y[i]-sum(X[i,j]*beta[j] for j=1:p))^2 for i=1:n) <= sse)\n",
    "    #@constraint(model, sum(beta[j]^2 for j=1:p)<=reg)\n",
    "    @objective(model,Min, 1/n*sse + rho*sum(beta[j]^2 for j=1:p))\n",
    "\n",
    "    optimize!(model)\n",
    "    #println(\"Obj \", objective_value(model))\n",
    "    return value.(beta)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e484ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "master_primal_l2_ridge (generic function with 3 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "function master_primal_l2(X0, Xt, y0, Dmin, Dmax, epsilon, delta, reg, ρ, ϵ_l2, δ_l2, β0_fix = false, β0_val = 0)\n",
    "\n",
    "    M = 1000\n",
    "    n0, p = size(X0)\n",
    "    n, _ = size(Xt)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, β[i=1:n,j=1:p])\n",
    "    @variable(model, b[i=1:n]>=0)\n",
    "    @variable(model, β0[j=1:p])\n",
    "    @variable(model, z[i=1:n], Bin)\n",
    "    @variable(model, y[i=1:n])\n",
    "    # Add objective\n",
    "    @objective(model, Min, 1/n0*sum((y0[i]-sum(X0[i,j]*β0[j] for j=1:p))^2 for i=1:n0)\n",
    "        + reg*sum(b[i] for i=1:n) + ρ*sum(β0[j]^2 for j=1:p))\n",
    "\n",
    "    #@constraint(model,[i=1:n], y .- dot(X,β) .<= b)\n",
    "    #@constraint(model,[i=1:n],-y .+ dot(X,β) .<= b)\n",
    "\n",
    "    if β0_fix\n",
    "        @constraint(model, β0 .== β0_val)\n",
    "    end\n",
    "    @constraint(model, res_plus_min[i=1:n],  - y[i] + dot(Xt[i,:],β[i,:]) <= b[i])\n",
    "    @constraint(model, res_minus_min[i=1:n],  y[i] - dot(Xt[i,:],β[i,:]) <= b[i])\n",
    "\n",
    "    @constraint(model,  y .== Dmax.*z + Dmin.*(1 .-z))\n",
    "    #@constraint(model,  y .== Dmax.*(1 .-z) + Dmin.*z)\n",
    "\n",
    "    #@constraint(model, bigM[i=1:n], (dot(Xt[i,:],β[i,:])-Dmin[i])^2 + M*z[i] >= (dot(Xt[i,:],β[i,:])-Dmax[i])^2)\n",
    "    @constraint(model, bigM[i=1:n], (dot(Xt[i,:],β[i,:]) - Dmin[i])^2 - (dot(Xt[i,:],β[i,:])-Dmax[i])^2 + M*z[i] >= 0)\n",
    "\n",
    "\n",
    "    #@constraint(model, bigM2[i=1:n],  (dot(Xt[i,:],β[i,:])-Dmax[i])^2 + M*(1-z[i]) >= (dot(Xt[i,:],β[i,:])-Dmin[i])^2)\n",
    "\n",
    "    @constraint(model, diff_plus[i=2:n],   β[i,:] .- β[i-1,:] .<= delta)\n",
    "    @constraint(model, diff_minus[i=2:n], - β[i,:] .+ β[i-1,:] .<= delta)\n",
    "\n",
    "    @constraint(model, diff_0_plus[i=1:n],   β[i,:] .- β0 .<= epsilon)\n",
    "    @constraint(model, diff_0_minus[i=1:n], - β[i,:] .+ β0 .<= epsilon)\n",
    "\n",
    "    @constraint(model, sq_0[i=1:n], sum((β[i,:] .- β0).^2) .<= ϵ_l2)\n",
    "    @constraint(model, sq_t[i=2:n], sum((β[i,:] .- β[i-1,:]).^2) .<= δ_l2)\n",
    "\n",
    "    #@constraint(model, abs_0[i=1:n], sum(abs.(β[i,:] .- β0)) .<= ϵ_l1)\n",
    "    #@constraint(model, abs_t[i=2:n], sum(abs.(β[i,:] .- β[i-1,:])) .<= δ_l1)\n",
    "\n",
    "    optimize!(model);\n",
    "    #println(\"SUM \", sum(getvalue.(b)))\n",
    "    #println(\"Y \", getvalue.(y))\n",
    "    return objective_value(model), getvalue.(β), getvalue.(β0)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function master_primal_l2_ridge(X0, Xt, y0, Dmin, Dmax, epsilon, delta, reg, ρ, ϵ_l2, δ_l2, β0_fix = false, β0_val = 0)\n",
    "\n",
    "    M = 1000\n",
    "    n0, p = size(X0)\n",
    "    n, _ = size(Xt)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "    # Add variables\n",
    "    @variable(model, β[i=1:n,j=1:p])\n",
    "    @variable(model, β0[j=1:p])\n",
    "    @variable(model, z[i=1:n], Bin)\n",
    "    @variable(model, y[i=1:n])\n",
    "    # Add objective\n",
    "    @objective(model, Min, 1/n0*sum((y0[i]-sum(X0[i,j]*β0[j] for j=1:p))^2 for i=1:n0)\n",
    "        + reg*sum((y[i] - dot(Xt[i,:],β[i,:]))^2 for i=1:n) + ρ*sum(β0[j]^2 for j=1:p))\n",
    "\n",
    "    #@constraint(model,[i=1:n], y .- dot(X,β) .<= b)\n",
    "    #@constraint(model,[i=1:n],-y .+ dot(X,β) .<= b)\n",
    "\n",
    "    if β0_fix\n",
    "        @constraint(model, β0 .== β0_val)\n",
    "    end\n",
    "\n",
    "    @constraint(model,  y .== Dmax.*z + Dmin.*(1 .-z))\n",
    "    #@constraint(model,  y .== Dmax.*(1 .-z) + Dmin.*z)\n",
    "\n",
    "    #@constraint(model, bigM[i=1:n], (dot(Xt[i,:],β[i,:])-Dmin[i])^2 + M*z[i] >= (dot(Xt[i,:],β[i,:])-Dmax[i])^2)\n",
    "    @constraint(model, bigM[i=1:n], (dot(Xt[i,:],β[i,:]) - Dmin[i])^2 - (dot(Xt[i,:],β[i,:])-Dmax[i])^2 + M*z[i] >= 0)\n",
    "\n",
    "\n",
    "    #@constraint(model, bigM2[i=1:n],  (dot(Xt[i,:],β[i,:])-Dmax[i])^2 + M*(1-z[i]) >= (dot(Xt[i,:],β[i,:])-Dmin[i])^2)\n",
    "\n",
    "    @constraint(model, diff_plus[i=2:n],   β[i,:] .- β[i-1,:] .<= delta)\n",
    "    @constraint(model, diff_minus[i=2:n], - β[i,:] .+ β[i-1,:] .<= delta)\n",
    "\n",
    "    @constraint(model, diff_0_plus[i=1:n],   β[i,:] .- β0 .<= epsilon)\n",
    "    @constraint(model, diff_0_minus[i=1:n], - β[i,:] .+ β0 .<= epsilon)\n",
    "\n",
    "    @constraint(model, sq_0[i=1:n], sum((β[i,:] .- β0).^2) .<= ϵ_l2)\n",
    "    @constraint(model, sq_t[i=2:n], sum((β[i,:] .- β[i-1,:]).^2) .<= δ_l2)\n",
    "\n",
    "    #@constraint(model, abs_0[i=1:n], sum(abs.(β[i,:] .- β0)) .<= ϵ_l1)\n",
    "    #@constraint(model, abs_t[i=2:n], sum(abs.(β[i,:] .- β[i-1,:])) .<= δ_l1)\n",
    "\n",
    "    optimize!(model);\n",
    "    #println(\"SUM \", sum(getvalue.(b)))\n",
    "    #println(\"Y \", getvalue.(y))\n",
    "    return objective_value(model), getvalue.(β), getvalue.(β0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ecae3",
   "metadata": {},
   "source": [
    "# Linear rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69b65ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adaptive_ridge_regression_standard (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function adaptive_ridge_regression_standard(X, y, ρ_β0, ρ_V0, T)\n",
    "    \n",
    "#     Adaptive ridge: does not run fast\n",
    "    \n",
    "\n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "    X, Z, y = get_X_Z_y(X, y, T)\n",
    "\n",
    "    N, P = size(X)\n",
    "    # Add variables\n",
    "\n",
    "    @variable(model, β0[j=1:P])\n",
    "    @variable(model, V0[j=1:P, k=1:T*P+T])\n",
    "    @variable(model, t>=0)\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Min, t + ρ_β0 * sum(β0[j]^2 for j=1:P)\n",
    "                            + ρ_V0 * sum(V0[j,k]^2 for j=1:P for k=1:T*P+T)\n",
    "    )\n",
    "\n",
    "    @constraint(model, t >= 1 / N * sum((y[i] - sum(X[i, j] * (β0[j]\n",
    "                 + sum(V0[j, l] * Z[i, l] for l=1:(T * P + T))\n",
    "                 )\n",
    "                for j=1:P))^2\n",
    "        for i=1:N))\n",
    "\n",
    "    optimize!(model);\n",
    "    return objective_value(model), getvalue.(β0), getvalue.(V0), Z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "71f6b698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adaptive_ridge_regression_before_exact (generic function with 1 method)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function adaptive_ridge_regression_before_exact(X, y, ρ_β0, ρ_V0, T)\n",
    "    \n",
    "    #Version with actual robust equivalence\n",
    "    #We use beta t\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 1)\n",
    "    X, Z, y = get_X_Z_y(X, y, T)\n",
    "\n",
    "    N, P = size(X)\n",
    "    # Add variables\n",
    "\n",
    "    @variable(model, β0[j=1:P])\n",
    "    @variable(model, V0[j=1:P, k=1:T*P+T])\n",
    "    @variable(model, t>=0)\n",
    "    @variable(model, β[t=1:N,j=1:P])\n",
    "\n",
    "    # Add objective\n",
    "    @objective(model, Min, t + ρ_β0 * sum(β0[j]^2 for j=1:P)\n",
    "                            + ρ_V0 * sum(V0[j,k]^2 for j=1:P for k=1:T*P+T)\n",
    "    )\n",
    "\n",
    "    @constraint(model, t >= 1 / N * sum((y[i] - sum(transpose(X[i, :])*β[i,:]))^2 for i=1:N))\n",
    "        \n",
    "    @constraint(model, [i=1:N], β[i,:] .== β0+V0*Z[i,:])\n",
    "\n",
    "    optimize!(model);\n",
    "    return objective_value(model), getvalue.(β0), getvalue.(V0), getvalue.(β)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "772fd01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adaptive_ridge_regression_exact (generic function with 2 methods)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function adaptive_ridge_regression_exact(X, y, ρ_β0, ρ_V0, T, N0)\n",
    "    \n",
    "    #Version with actual robust equivalence\n",
    "    #The formula for the regularization is different\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(with_optimizer(Gurobi.Optimizer, GRB_ENV))\n",
    "    set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "    X, Z, y = get_X_Z_y(X, y, T)\n",
    "\n",
    "    N, P = size(X)\n",
    "    # Add variables\n",
    "\n",
    "    @variable(model, β0[j=1:P])\n",
    "    @variable(model, V0[j=1:P, k=1:T*P+T])\n",
    "    @variable(model, t>=0)\n",
    "    @variable(model, β[t=1:N,j=1:P])\n",
    "\n",
    "    # If no stable part, then no reg\n",
    "    if N0 == 1\n",
    "        ρ_β0 = 0\n",
    "    end\n",
    "    \n",
    "    # Add objective\n",
    "    @objective(model, Min, t + 1/N0*ρ_β0 * sum(β0[j]^2 for j=1:P)\n",
    "                            + 1/(N-N0)*ρ_V0 * sum(β[t,k]^2 for t=1:N for k=1:P)\n",
    "    )\n",
    "\n",
    "    @constraint(model, t >= 1 / N * sum((y[i] - sum(transpose(X[i, :])*β[i,:]))^2 for i=1:N))\n",
    "    \n",
    "    #stable and adaptive part\n",
    "    @constraint(model, [i=N0:N], β[i,:] .== β0+V0*Z[i,:])\n",
    "    @constraint(model, [i=1:N0-1], β[i,:] .== β0)\n",
    "\n",
    "    optimize!(model);\n",
    "    return objective_value(model), getvalue.(β0), getvalue.(V0), getvalue.(β)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb71aa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_X_Z_y (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_X_Z_y(X, y, T)\n",
    "    \"\"\"\n",
    "    Input: training data X and corresponding labels y ; T: how many time-steps from the past to be used\n",
    "    Output: the past features X with past targets y as a Z training data (no present features)\n",
    "    \"\"\"\n",
    "    n, p = size(X)\n",
    "    #T past time steps * p features + T targets\n",
    "    Z = ones(n-T, T*p+T)\n",
    "    for i=T+1:n\n",
    "        for t=1:T\n",
    "            Z[i-T,1+p*(t-1):p*t] = X[i-t,:]\n",
    "        end\n",
    "        Z[i-T, (p*T+1):end] = y[i-T:i-1]\n",
    "    end\n",
    "    return X[T+1:end,:], Z, y[T+1:end]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c2e09a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.953041262548516 0.4782870249686575; 1.0535457742742587 0.334867774903859], [0.6507019593290054 0.4323663266396612 … -0.24864108988725883 0.4349146279684725; 0.953041262548516 0.4782870249686575 … 0.4349146279684725 0.7909332310183326], [0.7909332310183326, 0.7909332310183326])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e, Z, f = get_X_Z_y(X[1:5,1:2], y[1:5], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3b08d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019454798289416986\n"
     ]
    }
   ],
   "source": [
    "obj, β0, V0, Z = adaptive_ridge_regression_standard(X[1:100,1:3], y[1:100], 0.1, 0.1, 3)\n",
    "println(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "14aca550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 0.1504249753800681\n",
       " 0.4548341433068424\n",
       " 0.14489314118672222"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "β0+V0*Z[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "01ea2fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5860827402650841\n"
     ]
    }
   ],
   "source": [
    "obj, β0, V0, β = adaptive_ridge_regression_exact(X[1:100,1:3], y[1:100], 0, 0.1, 3, 16)\n",
    "println(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2f73bc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5852809828471135\n"
     ]
    }
   ],
   "source": [
    "obj, β0, V0, β = adaptive_ridge_regression_exact(X[1:100,1:3], y[1:100], 0, 0.1, 3, 0)\n",
    "println(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "55ad112e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 9.1.2 build v9.1.2rc0 (mac64)\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 291 rows, 331 columns and 4074 nonzeros\n",
      "Model fingerprint: 0x6c09b65a\n",
      "Model has 39 quadratic objective terms\n",
      "Model has 1 quadratic constraint\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-02, 3e+00]\n",
      "  QMatrix range    [1e-06, 8e-02]\n",
      "  QLMatrix range   [6e-05, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  QObjective range [2e-01, 2e-01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [0e+00, 0e+00]\n",
      "  QRHS range       [8e-01, 8e-01]\n",
      "Presolve time: 0.00s\n",
      "Presolved: 391 rows, 432 columns, 4759 nonzeros\n",
      "Presolved model has 2 second-order cone constraints\n",
      "Ordering time: 0.00s\n",
      "\n",
      "Barrier statistics:\n",
      " Dense cols : 41\n",
      " Free vars  : 291\n",
      " AA' NZ     : 4.854e+03\n",
      " Factor NZ  : 1.827e+04\n",
      " Factor Ops : 8.039e+05 (less than 1 second per iteration)\n",
      " Threads    : 1\n",
      "\n",
      "                  Objective                Residual\n",
      "Iter       Primal          Dual         Primal    Dual     Compl     Time\n",
      "   0   3.11421765e+00  5.36029178e-01  1.07e+00 2.06e-01  1.66e-02     0s\n",
      "   1   1.24866824e+00 -1.49454176e-01  1.71e-06 3.65e-07  9.92e-03     0s\n",
      "   2   8.61394373e-02  2.04473484e-03  5.59e-08 8.84e-08  5.96e-04     0s\n",
      "   3   3.58614220e-02  4.45432877e-03  1.72e-08 9.75e-14  2.23e-04     0s\n",
      "   4   2.59765144e-02  1.04984114e-02  4.71e-09 5.29e-14  1.10e-04     0s\n",
      "   5   2.04917687e-02  1.25069818e-02  1.27e-09 3.85e-15  5.66e-05     0s\n",
      "   6   2.02737134e-02  1.76733783e-02  8.11e-10 5.52e-15  1.84e-05     0s\n",
      "   7   1.97504484e-02  1.93025152e-02  2.43e-14 7.55e-15  3.18e-06     0s\n",
      "   8   1.94566600e-02  1.94498483e-02  2.57e-09 2.24e-09  4.84e-08     0s\n",
      "   9   1.94547804e-02  1.94546020e-02  2.00e-09 5.52e-11  1.27e-09     0s\n",
      "\n",
      "Barrier solved model in 9 iterations and 0.11 seconds\n",
      "Optimal objective 1.94547804e-02\n",
      "\n",
      "\n",
      "User-callback calls 62, time in user-callback 0.00 sec\n",
      "0.019454780357772428\n",
      "[0.1504354548405991, 0.45487816131635345, 0.14490341051823682]\n",
      "[0.15043545467982017, 0.4548781608300493, 0.14490341036312396]\n"
     ]
    }
   ],
   "source": [
    "obj, β0, V0, β = adaptive_ridge_regression_before_exact(X[1:100,1:3], y[1:100], 0.1, 0.1, 3)\n",
    "println(obj)\n",
    "println(β0+V0*Z[1,:])\n",
    "println(β[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "002f9b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 9.1.2 build v9.1.2rc0 (mac64)\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 291 rows, 331 columns and 4074 nonzeros\n",
      "Model fingerprint: 0xac638d92\n",
      "Model has 39 quadratic objective terms\n",
      "Model has 1 quadratic constraint\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-02, 3e+00]\n",
      "  QMatrix range    [2e-01, 1e+01]\n",
      "  QLMatrix range   [6e-01, 4e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  QObjective range [2e-01, 2e-01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [0e+00, 0e+00]\n",
      "  QRHS range       [8e-01, 8e-01]\n",
      "Presolve removed 291 rows and 291 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 42 rows, 83 columns, 864 nonzeros\n",
      "Presolved model has 2 second-order cone constraints\n",
      "Ordering time: 0.00s\n",
      "\n",
      "Barrier statistics:\n",
      " AA' NZ     : 8.600e+02\n",
      " Factor NZ  : 9.030e+02\n",
      " Factor Ops : 2.558e+04 (less than 1 second per iteration)\n",
      " Threads    : 1\n",
      "\n",
      "                  Objective                Residual\n",
      "Iter       Primal          Dual         Primal    Dual     Compl     Time\n",
      "   0   3.61515699e+00  7.44923934e-01  3.25e-01 3.18e+00  7.19e-02     0s\n",
      "   1   6.42215881e-01  3.85814230e-01  3.57e-07 5.34e-01  1.06e-02     0s\n",
      "   2   5.73290854e-02 -4.33568773e-02  1.09e-08 7.66e-03  1.33e-03     0s\n",
      "   3   2.49646918e-02  1.65629209e-02  3.98e-13 4.86e-04  1.09e-04     0s\n",
      "   4   2.00952502e-02  1.91575779e-02  1.61e-13 5.35e-10  1.13e-05     0s\n",
      "   5   1.94626577e-02  1.94368039e-02  2.76e-12 1.48e-11  3.11e-07     0s\n",
      "   6   1.94554365e-02  1.94518164e-02  3.70e-12 1.78e-12  4.36e-08     0s\n",
      "   7   1.94547983e-02  1.94547552e-02  7.29e-10 9.21e-13  5.15e-10     0s\n",
      "\n",
      "Barrier solved model in 7 iterations and 0.02 seconds\n",
      "Optimal objective 1.94547983e-02\n",
      "\n",
      "\n",
      "User-callback calls 56, time in user-callback 0.00 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 0.1504249753800681\n",
       " 0.45483414330684235\n",
       " 0.14489314118672222"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, β0, V0, β, X1, Z1 = adaptive_ridge_regression_exact_test(X[1:100,1:3], y[1:100], 0.1, 0.1, 3)\n",
    "β[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2e470d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.533767797757831"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, P = size(X1)\n",
    "i = 1\n",
    "T = 3\n",
    "sum(X1[i, j] * (β0[j]+ sum(V0[j, l] * Z1[i, l] for l=1:(T * P + T))) for j=1:P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "87684819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.533767797757831"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose(X1[i,:])*β[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "25386963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1504249753800681\n",
      "[0.1504249753800681, 0.4548341433068424, 0.14489314118672222]\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "println(β0[j]+sum(V0[j, l] * Z[i, l] for l=1:(T * P + T)))\n",
    "println(β0+V0*Z[i,:])\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a313920b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_adaptive (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluate_adaptive(X, y, y_true, β0, V0, T)\n",
    "    X, Z, y = get_X_Z_y(X, y, T)\n",
    "    \n",
    "    N, P = size(X)\n",
    "\n",
    "    pred = [sum(X[i, j] * (β0[j] + sum(V0[j, :] .* Z[i, :])) for j=1:P) for i=1:N]\n",
    "    if size(y_true) == size(pred)\n",
    "        err = [abs(y_true[i]-pred[i]) for i=1:N]\n",
    "    else\n",
    "        y_true = y_true[T+1:end]\n",
    "        err = [abs(y_true[i]-pred[i]) for i=1:N]\n",
    "    end\n",
    "    return err\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bbb311d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_adaptive_retrained (generic function with 1 method)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluate_adaptive_retrained(X, y, y_true, β0_list, V0_list, T)\n",
    "    X, Z, y = get_X_Z_y(X, y, T)\n",
    "    \n",
    "    N, P = size(X)\n",
    "\n",
    "    pred = [sum(X[i, j] * (β0_list[i,j] + sum(V0_list[i, j, :] .* Z[i, :])) for j=1:P) for i=1:N]\n",
    "    if size(y_true) == size(pred)\n",
    "        err = [abs(y_true[i]-pred[i]) for i=1:N]\n",
    "    else\n",
    "        y_true = y_true[T+1:end]\n",
    "        err = [abs(y_true[i]-pred[i]) for i=1:N]\n",
    "    end\n",
    "    return err\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "1a0ebb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2772791481270517, -0.22139104249186187, 0.25353125721465, 0.13052228762534424, 0.16184232903639695, 0.02133275894336177, -0.23829147212091226, 0.14926772611852196, 0.28826028113700647, 0.7460128800839689  …  0.6974694989745849, 0.4334507651952026, 0.4535167025297599, 0.4228111533412082, -0.9470370564072363, -0.004111494312688896, 0.12274140243903106, 0.08108513878897414, -1.4025705199739362, 0.08251350395763898], [0.7111257347162706, 0.768332857553357, 0.1994216466416816, 0.6812973681052467, 0.08794262534614797, 0.24054297764516547, 0.02174221306394139, 0.4729100683816176, 0.04127284032870793, 0.5794229469798875  …  0.29188476681020137, 0.3992553152476475, 0.02696830662909544, 0.5024620796474668, 0.7050416471102421, 0.23408638321844036, 0.4254973199898675, 0.7075806393798176, 0.9146849288371715, 0.6178682421677187])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, err = evaluate_adaptive(X[1:100,1:3], y[1:100], y[4:100], β0, V0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8059e02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval_method (generic function with 2 methods)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function eval_method(X, y, y_true, split_, past, num_past, val, uncertainty, ϵ_inf, δ_inf, last_yT,\n",
    "        ϵ_l2, δ_l2, ρ, reg, max_cuts, verbose,\n",
    "        fix_β0, more_data_for_β0, benders, ridge, linear_adapt)\n",
    "\n",
    "    threshold_benders = 0.01\n",
    "    n, p = size(X)\n",
    "    split_index = floor(Int,n*split_)\n",
    "    #TODO change spli_index with max(split inex, 1)\n",
    "    X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, split_index-num_past*past+1, num_past*past, val, uncertainty, last_yT)\n",
    "\n",
    "    β_list0 = zeros(val, p)\n",
    "    β_listt = zeros(val, p)\n",
    "    β_listl2 = zeros(val, p)\n",
    "    β_l2_init = l2_regression(X0,y0,ρ);\n",
    "    β0_list_linear_adapt = zeros(val, p)\n",
    "    V0_list_linear_adapt = zeros(val, p, (past-1)*p+past-1)\n",
    "    err = ones(val)\n",
    "    \n",
    "    ###Linear decision rule\n",
    "    _, β0_0, V0_0 = adaptive_ridge_regression_standard(X0, y0, ρ, ρ, past-1)\n",
    "    print(size(V0))\n",
    "    \n",
    "    for s=1:val\n",
    "        #TODO Check how much I use from the past\n",
    "        if more_data_for_β0\n",
    "            X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, split_index-num_past*past+1, s+(num_past-1)*past, past-1, uncertainty, last_yT)\n",
    "        else\n",
    "            X0, y0, Xt, yt, yt_true, D_min, D_max = prepare_data_from_y(X, y, s+split_index-num_past*past+1, (num_past-1)*past, past-1, uncertainty, last_yT)\n",
    "        end\n",
    "\n",
    "        if benders\n",
    "            ##TODO handle fix_beta0\n",
    "            obj, β0_val, α, y_val = master_problem(X0, Xt, y0, D_min, D_max, threshold_benders, ϵ_inf, δ_inf, reg, ρ, max_cuts, verbose)\n",
    "            _, βt_val = S_primal(Xt, y_val, β0_val, ϵ_inf, δ_inf);\n",
    "        else\n",
    "            if ridge\n",
    "                obj, βt_val, β0_val = master_primal_l2_ridge(X0, Xt, y0, D_min, D_max, ϵ_inf, δ_inf, reg, ρ, ϵ_l2, δ_l2, fix_β0, β_l2_init)\n",
    "            else\n",
    "                obj, βt_val, β0_val = master_primal_l2(X0, Xt, y0, D_min, D_max, ϵ_inf, δ_inf, reg, ρ, ϵ_l2, δ_l2, fix_β0, β_l2_init)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if linear_adapt\n",
    "            #err[s] = evaluate_adaptive(vcat(X0[end-past+2:end,:],Xt), vcat(y0[end-past+2:end,:],yt), yt_true, β0, V0, past)\n",
    "            _, β0, V0 = adaptive_ridge_regression_standard(vcat(X0,Xt), vcat(y0,yt), ρ, ρ, past-1)\n",
    "            β0_list_linear_adapt[s,:] = β0\n",
    "            V0_list_linear_adapt[s,:,:] = V0\n",
    "            N = size(X0)[1]\n",
    "        end\n",
    "        \n",
    "        β_listt[s,:] = βt_val[past-1,:]\n",
    "        β_list0[s,:] = β0_val\n",
    "        β_l2 = l2_regression(vcat(X0,Xt),vcat(y0,yt),ρ);\n",
    "        β_listl2[s,:] = β_l2\n",
    "\n",
    "    end\n",
    "\n",
    "    X0, y0, Xt, yt, _, D_min, D_max = prepare_data_from_y(X, y, 1, split_index, val, uncertainty, last_yT)\n",
    "    _, _, _, _, yt_true, _, _ = prepare_data_from_y(X, y_true, 1, split_index, val, uncertainty, last_yT)\n",
    "\n",
    "    N = size(X0)[1]\n",
    "    err_linear_rule = evaluate_adaptive(vcat(X0[N-past+2:end,:],Xt), vcat(y0[N-past+2:end,:],yt), yt_true, β0_0, V0_0, past-1)\n",
    "    err_linear_rule_retrained = evaluate_adaptive_retrained(vcat(X0[N-past+2:end,:],Xt), vcat(y0[N-past+2:end,:],yt), \n",
    "        yt_true, β0_list_linear_adapt, V0_list_linear_adapt, past-1)\n",
    "    \n",
    "    err_0 = [abs(yt_true[s]-dot(Xt[s,:],β_list0[s,:])) for s=1:val]\n",
    "    err_t = [abs(yt_true[s]-dot(Xt[s,:],β_listt[s,:])) for s=1:val]\n",
    "    err_baseline = [abs(yt_true[s]-dot(Xt[s,:],β_l2_init)) for s=1:val]\n",
    "    err_l2 = [abs(yt_true[s]-dot(Xt[s,:],β_listl2[s,:])) for s=1:val]\n",
    "\n",
    "    println(\"\\n### β0 Baseline ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_baseline))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_baseline, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_baseline, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_baseline, yt_true))\n",
    "\n",
    "    println(\"\\n### β0 Baseline Retrained ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_l2))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_l2, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_l2, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_l2, yt_true))\n",
    "    \n",
    "    println(\"\\n### β0 V0 Linear rule Adaptive ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_linear_rule))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_linear_rule, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_linear_rule, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_linear_rule, yt_true))\n",
    "    \n",
    "    println(\"\\n### β0 V0 Linear rule Adaptive Retrained ###\")\n",
    "    println(\"MAE Baseline: \", mean(err_linear_rule_retrained))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_linear_rule_retrained, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_linear_rule_retrained, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_linear_rule_retrained, yt_true))\n",
    "\n",
    "    println(\"\\n### β0 Adaptive ###\")\n",
    "    println(\"MAE 0: \", mean(err_0))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_0, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_0, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_0, yt_true))\n",
    "\n",
    "    println(\"\\n### βt Adaptive ###\")\n",
    "    println(\"MAE t: \", mean(err_t))\n",
    "    println(\"CVAR 0.05 :\", compute_CVaR(err_t, 0.05))\n",
    "    println(\"CVAR 0.15 :\", compute_CVaR(err_t, 0.15))\n",
    "    println(\"R2 : \", R2_err(err_t, yt_true))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f1f231c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12)"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "Gurobi Error 10020: Constraint Q not PSD (diagonal adjustment of 1.1e-05 would be required). Set NonConvex parameter to 2 to solve model.",
     "output_type": "error",
     "traceback": [
      "Gurobi Error 10020: Constraint Q not PSD (diagonal adjustment of 1.1e-05 would be required). Set NonConvex parameter to 2 to solve model.",
      "",
      "Stacktrace:",
      "  [1] _check_ret",
      "    @ ~/.julia/packages/Gurobi/BAtIN/src/MOI_wrapper/MOI_wrapper.jl:326 [inlined]",
      "  [2] _check_ret_GRBoptimize(model::Gurobi.Optimizer)",
      "    @ Gurobi ~/.julia/packages/Gurobi/BAtIN/src/MOI_wrapper/MOI_wrapper.jl:343",
      "  [3] optimize!(model::Gurobi.Optimizer)",
      "    @ Gurobi ~/.julia/packages/Gurobi/BAtIN/src/MOI_wrapper/MOI_wrapper.jl:2672",
      "  [4] optimize!(b::MathOptInterface.Bridges.LazyBridgeOptimizer{Gurobi.Optimizer})",
      "    @ MathOptInterface.Bridges /Users/iai/builds/InterpretableAI/SystemImage/SysImgBuilder/.julia/packages/MathOptInterface/YDdD3/src/Bridges/bridge_optimizer.jl:319",
      "  [5] optimize!(m::MathOptInterface.Utilities.CachingOptimizer{MathOptInterface.AbstractOptimizer, MathOptInterface.Utilities.UniversalFallback{MathOptInterface.Utilities.GenericModel{Float64, MathOptInterface.Utilities.ModelFunctionConstraints{Float64}}}})",
      "    @ MathOptInterface.Utilities /Applications/Julia-1.6.app/Contents/Resources/julia/lib/julia/sys.dylib:-1",
      "  [6] optimize!(model::Model, optimizer_factory::Nothing; bridge_constraints::Bool, ignore_optimize_hook::Bool, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ JuMP /Applications/Julia-1.6.app/Contents/Resources/julia/lib/julia/sys.dylib:-1",
      "  [7] optimize! (repeats 2 times)",
      "    @ /Users/iai/builds/InterpretableAI/SystemImage/SysImgBuilder/.julia/packages/JuMP/qhoVb/src/optimizer_interface.jl:106 [inlined]",
      "  [8] adaptive_ridge_regression_standard(X::Matrix{Float64}, y::Vector{Float64}, ρ_β0::Float64, ρ_V0::Float64, T::Int64)",
      "    @ Main ./In[24]:29",
      "  [9] eval_method(X::DataFrame, y::Vector{Float64}, y_true::Vector{Float64}, split_::Float64, past::Int64, num_past::Int64, val::Int64, uncertainty::Float64, ϵ_inf::Float64, δ_inf::Float64, last_yT::Bool, ϵ_l2::Float64, δ_l2::Float64, ρ::Float64, reg::Float64, max_cuts::Int64, verbose::Bool, fix_β0::Bool, more_data_for_β0::Bool, benders::Bool, ridge::Bool, linear_adapt::Bool)",
      "    @ Main ./In[196]:45",
      " [10] top-level scope",
      "    @ In[198]:26",
      " [11] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [12] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "linear_adapt = true\n",
    "split_ = N0/(N0+Nt)\n",
    "past = 5\n",
    "num_past = 20#floor(Int, Nt/past)\n",
    "\n",
    "last_yT = false\n",
    "max_cuts = 10\n",
    "verbose = false\n",
    "\n",
    "uncertainty = 0.#1\n",
    "δ_inf = 0.02\n",
    "ϵ_inf = 0.02\n",
    "\n",
    "ϵ_l2 = 0.05\n",
    "δ_l2 = 0.05\n",
    "\n",
    "reg = 1/(past*num_past)\n",
    "ρ = 0.1\n",
    "val = Nt-1; #n-split_index;\n",
    "\n",
    "fix_β0 = false\n",
    "more_data_for_β0 = false\n",
    "benders = false\n",
    "ridge = true\n",
    "\n",
    "eval_method(X, y, y_true, split_, past, num_past, val, uncertainty, ϵ_inf, δ_inf, last_yT, \n",
    "        ϵ_l2, δ_l2, ρ, reg, max_cuts, verbose, \n",
    "        fix_β0, more_data_for_β0, benders, ridge, linear_adapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8f815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
